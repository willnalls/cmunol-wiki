version=pmwiki-2.2.130 ordered=1 urlencoded=1
agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1 Safari/605.1.15
author=CharlieP
charset=UTF-8
csum=
ctime=1617042227
host=74.109.251.161
name=Articles.LinguisticDiversityInAIApplications
rev=3
targets=GradebookArticles.LinguisticDiversityInAIApplications,Articles.LinguisticDiversityInAIApplications,Category.Varia
text=(:if authgroup @tas:)%0a%0a(:toggle id=gi show="show grading interface" hide="hide grading interface":)%0a%0a>>id=gi%3c%3c%0a%0a[[GradebookArticles.{$Name}|See article in gradebook]] \\%0a[[{$FullName}?action=diff|See all changes to article]]%0a%0a(:foxform Site.FoxForms#gradeitem:)%0a%0a(:foxform Site.FoxForms#gradingcomment:)%0a%0a>>%3c%3c%0a%0a----%0a%0a(:ifend:)%0a%0a[[!Varia]]%0a%0a!Linguistic Diversity in AI Applications%0a%0a:Author: AlexisA%0a%0a'''Summary:''' \\%0a%0a[[#summary]]%0a%0aIn order to ensure that natural language processing applications like Siri are as broadly useful as possible, developers need to account for the linguistic diversity of their users.%0a%0a[[#summaryends]]%0a%0a----%0a[[#content]]%0a%0aWe've seen a lot in the news over the past few years about how facial recognition technology and other applications of AI do not adequately take into account racial and other forms of diversity in their development (http://ctdebate.org/PDFs/CDAPacketOct19.pdf has a lot of good sources on FRT specifically). The same, however, applies to Natural Language Processing, since it is a form of artificial intelligence and the development of its linguistic models is highly dependent on the samples it is given.%0aThroughout this class, we've learned a lot about the wide variety of dialects and the many different ways that grammatical information can be communicated in languages. If the developers of a language program like Siri have a prescriptivist view on language (either their own or other languages they are developing for), their technology may not be as useful for speakers of other dialects. They need to be active in seeking out testers from a wide variety of linguistic backgrounds to ensure that their product will be useful to as many people as possible.%0aTo provide a concrete example of this, I knew someone who bought the iPhone 4s when it first came out in 2011 (and was still very expensive) specifically for Siri. However, she found that Siri could not understand her Russian accent at all, making her purchase completely useless. Over time, it seems that the developers of Siri became aware of this oversight and have made a lot of progress in adapting their program for many linguistic backgrounds.%0a%0a[[#contentends]]%0a----%0a----%0a%0a!!Expansions:%0a[[#expansions]]%0a#foxbegin 210508-020047-469700#%0a(:div1 class=expansionhead:)%0a!!!Companies working to reduce the accent gap in AI Applications %0a-> - CharlieP%0a(:div1end:) %0a>>messageitem%3c%3c %0aAs was pointed out, the reason behind the trouble of AI recognizing accents is because of the limited datasets available for developers to train on. This is largely due to a lack of financial incentive and trouble finding speakers to build a dataset. However, with the rapid rise of AI speech recognition, increasing diversity and robustness of existing AI systems is becoming more important, as well. Some companies have started to take note, with Cambridge's Speechmatics specializing entirely in generating a corpus of English from speakers all over the world. The corpus, named "Global English", uses thousands of hours of speech data from over 40 countries with tens of billions of words. I really think this is a large step forward in the development of speech recognition, and I could definitely see it become an industry standard to use such a data set and test with different speakers from all over the globe. Speechmatics recently launched on Microsoft Azure Marketplace, meaning that any type of developer can use their dataset and technology to develop a speech recognition system, which is a huge step forward in reducing the accent gap. %0a%0aSource: %0ahttps://www.speechmatics.com%0ahttps://www.speechtechmag.com/Articles/ReadArticle.aspx?ArticleID=145957%0a>>%3c%3c%0a#foxend 210508-020047-469700#%0a[[#expansionsend]]%0a%0a----%0a%25red%25 '''Add an expansion:'''%0a%0a(:if [ exists GradebookExpansions.{$Name}-{$Author} || equal {$Author} {$:Author} ] :)%0a%0a(:else:)%0a%0a(:foxform Site.FoxForms#newexpansion:)%0a%0a(:ifend:)%0a%0a----%0a----%0a%0a!!Comments%0a#foxbegin 210507-023716-341150#%0a(:div1 class=messagehead:)%0a>>rfloat%3c%3c   %0a[-06.05.2021 - 19:37-] &nbsp; %0a>>%3c%3c%0a(:if1 authgroup @tas:)%0a>>rfloat%3c%3c%0a{[foxdelrange button 210507-023716-341150 {$FullName} ]}%0a>>%3c%3c%0a(:if1end:)%0a!!!!!AlexanderW%0a(:div1end:) %0a>>messageitem%3c%3c %0a'''Trouble in selecting more diverse linguistical datasets'''%0a>>messageitem%3c%3c%0aI started to wonder why it seemed like understanding a variety of accents was so difficult for AI; as in, cannot the developers simply include more datasets involving different accents? Is there simply a lack of resources for that, or is it uneconomical for the companies to do so, or is it just negligence of the American-English speaking majority? %0a>>%3c%3c%0a#foxend 210507-023716-341150#%0a%0a%0a%0a(:section: A:)%0a(:Category: Varia:)
time=1620439246
author:1620439246=CharlieP
diff:1620439246:1620355035:=47,59c47%0a%3c #foxbegin 210508-020047-469700#%0a%3c (:div1 class=expansionhead:)%0a%3c !!!Companies working to reduce the accent gap in AI Applications %0a%3c -> - CharlieP%0a%3c (:div1end:) %0a%3c >>messageitem%3c%3c %0a%3c As was pointed out, the reason behind the trouble of AI recognizing accents is because of the limited datasets available for developers to train on. This is largely due to a lack of financial incentive and trouble finding speakers to build a dataset. However, with the rapid rise of AI speech recognition, increasing diversity and robustness of existing AI systems is becoming more important, as well. Some companies have started to take note, with Cambridge's Speechmatics specializing entirely in generating a corpus of English from speakers all over the world. The corpus, named "Global English", uses thousands of hours of speech data from over 40 countries with tens of billions of words. I really think this is a large step forward in the development of speech recognition, and I could definitely see it become an industry standard to use such a data set and test with different speakers from all over the globe. Speechmatics recently launched on Microsoft Azure Marketplace, meaning that any type of developer can use their dataset and technology to develop a speech recognition system, which is a huge step forward in reducing the accent gap. %0a%3c %0a%3c Source: %0a%3c https://www.speechmatics.com%0a%3c https://www.speechtechmag.com/Articles/ReadArticle.aspx?ArticleID=145957%0a%3c >>%3c%3c%0a%3c #foxend 210508-020047-469700#%0a---%0a> %0a
host:1620439246=74.109.251.161
author:1620355035=AlexanderW
diff:1620355035:1617042227:=65,82d64%0a%3c #foxbegin 210507-023716-341150#%0a%3c (:div1 class=messagehead:)%0a%3c >>rfloat%3c%3c   %0a%3c [-06.05.2021 - 19:37-] &nbsp; %0a%3c >>%3c%3c%0a%3c (:if1 authgroup @tas:)%0a%3c >>rfloat%3c%3c%0a%3c {[foxdelrange button 210507-023716-341150 {$FullName} ]}%0a%3c >>%3c%3c%0a%3c (:if1end:)%0a%3c !!!!!AlexanderW%0a%3c (:div1end:) %0a%3c >>messageitem%3c%3c %0a%3c '''Trouble in selecting more diverse linguistical datasets'''%0a%3c >>messageitem%3c%3c%0a%3c I started to wonder why it seemed like understanding a variety of accents was so difficult for AI; as in, cannot the developers simply include more datasets involving different accents? Is there simply a lack of resources for that, or is it uneconomical for the companies to do so, or is it just negligence of the American-English speaking majority? %0a%3c >>%3c%3c%0a%3c #foxend 210507-023716-341150#%0a
host:1620355035=73.90.192.150
author:1617042227=AlexisA
diff:1617042227:1617042227:=1,69d0%0a%3c (:if authgroup @tas:)%0a%3c %0a%3c (:toggle id=gi show="show grading interface" hide="hide grading interface":)%0a%3c %0a%3c >>id=gi%3c%3c%0a%3c %0a%3c [[GradebookArticles.{$Name}|See article in gradebook]] \\%0a%3c [[{$FullName}?action=diff|See all changes to article]]%0a%3c %0a%3c (:foxform Site.FoxForms#gradeitem:)%0a%3c %0a%3c (:foxform Site.FoxForms#gradingcomment:)%0a%3c %0a%3c >>%3c%3c%0a%3c %0a%3c ----%0a%3c %0a%3c (:ifend:)%0a%3c %0a%3c [[!Varia]]%0a%3c %0a%3c !Linguistic Diversity in AI Applications%0a%3c %0a%3c :Author: AlexisA%0a%3c %0a%3c '''Summary:''' \\%0a%3c %0a%3c [[#summary]]%0a%3c %0a%3c In order to ensure that natural language processing applications like Siri are as broadly useful as possible, developers need to account for the linguistic diversity of their users.%0a%3c %0a%3c [[#summaryends]]%0a%3c %0a%3c ----%0a%3c [[#content]]%0a%3c %0a%3c We've seen a lot in the news over the past few years about how facial recognition technology and other applications of AI do not adequately take into account racial and other forms of diversity in their development (http://ctdebate.org/PDFs/CDAPacketOct19.pdf has a lot of good sources on FRT specifically). The same, however, applies to Natural Language Processing, since it is a form of artificial intelligence and the development of its linguistic models is highly dependent on the samples it is given.%0a%3c Throughout this class, we've learned a lot about the wide variety of dialects and the many different ways that grammatical information can be communicated in languages. If the developers of a language program like Siri have a prescriptivist view on language (either their own or other languages they are developing for), their technology may not be as useful for speakers of other dialects. They need to be active in seeking out testers from a wide variety of linguistic backgrounds to ensure that their product will be useful to as many people as possible.%0a%3c To provide a concrete example of this, I knew someone who bought the iPhone 4s when it first came out in 2011 (and was still very expensive) specifically for Siri. However, she found that Siri could not understand her Russian accent at all, making her purchase completely useless. Over time, it seems that the developers of Siri became aware of this oversight and have made a lot of progress in adapting their program for many linguistic backgrounds.%0a%3c %0a%3c [[#contentends]]%0a%3c ----%0a%3c ----%0a%3c %0a%3c !!Expansions:%0a%3c [[#expansions]]%0a%3c %0a%3c [[#expansionsend]]%0a%3c %0a%3c ----%0a%3c %25red%25 '''Add an expansion:'''%0a%3c %0a%3c (:if [ exists GradebookExpansions.{$Name}-{$Author} || equal {$Author} {$:Author} ] :)%0a%3c %0a%3c (:else:)%0a%3c %0a%3c (:foxform Site.FoxForms#newexpansion:)%0a%3c %0a%3c (:ifend:)%0a%3c %0a%3c ----%0a%3c ----%0a%3c %0a%3c !!Comments%0a%3c %0a%3c %0a%3c %0a%3c (:section: A:)%0a%3c (:Category: Varia:)%0a\ No newline at end of file%0a
host:1617042227=128.237.82.1
