version=pmwiki-2.2.130 ordered=1 urlencoded=1
agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:87.0) Gecko/20100101 Firefox/87.0
author=AlexX
charset=UTF-8
csum=
ctime=1620428353
host=71.206.238.88
name=Articles.TheCultOfBERTology
rev=2
targets=GradebookArticles.TheCultOfBERTology,Articles.TheCultOfBERTology,Category.Varia
text=(:if authgroup @tas:)%0a%0a(:toggle id=gi show="show grading interface" hide="hide grading interface":)%0a%0a>>id=gi%3c%3c%0a%0a[[GradebookArticles.{$Name}|See article in gradebook]] \\%0a[[{$FullName}?action=diff|See all changes to article]]%0a%0a(:foxform Site.FoxForms#gradeitem:)%0a%0a(:foxform Site.FoxForms#gradingcomment:)%0a%0a>>%3c%3c%0a%0a----%0a%0a(:ifend:)%0a%0a[[!Varia]]%0a%0a!The Cult of BERTology%0a%0a:Author: AlexX%0a%0a'''Summary:''' \\%0a%0a[[#summary]]%0a%0aAn introduction to BERT, the current state-of-the-art in natural language processing.%0a%0a[[#summaryends]]%0a%0a----%0a[[#content]]%0a%0aBERT, short for Bidirectional Encoder Representations from Transformers is a deep learning technique that has enabled massive advances in NLP tasks in recent years. Introduced in 2018 by Google researchers, BERT immediately yielded top results on tasks ranging from question answering to natural language understanding and inference; in the years following, it has been shown to push the state-of-the-art in many more tasks, including word sense disambiguation and text summarization. %0a%0aBERT uses the transformer neural network architecture, the details of which will be omitted in this article (though http://nlp.seas.harvard.edu/2018/04/03/attention.html has a very good description of it). What truly distinguishes BERT from past techniques, however, is the manner in which it is (pre-)trained. Prior to being applied to any task, BERT is pre-trained on two tasks. The first is the Cloze task (or masked language modelling), in which certain words in a sentence are hidden from the reader and must be guessed from context. For instance, given the sentence "I like %3cmask> on my hotdogs," BERT might learn to replace the mask with "ketchup" or "mustard." The second task is next sentence prediction, in which the reader is given two sentences and has to decide whether the second sentence can logically follow the first. While these two tasks may seem fairly trivial to us, they go a long way toward enhancing BERT's ability to model natural language.%0a%0aWith that being said, researchers are still unsure as to why exactly BERT is so good at what it does. This has spawned an entire subfield of NLP known as "BERTology" dedicated to studying and explaining BERT. Interestingly, recent research has shown that BERT does have some concept of syntactic structure and consitutuency. For example, given the phrase "follow social media transitions on Capitol Hill," BERT's internal attentional mechanism learns to associate "transitions" more strongly with "media" than with "on," which reflects the fact that "media" and "transitions" are sisters in the syntax tree while "transitions" and "on" are not. Further, it has been shown that BERT "understands" agreement - for example, given a sentence like "The game %3cmask> bad," more often than not, BERT is able to correctly "is" rather than "are" to fill the mask.%0a%0aOne caveat of all this is that as a language model, BERT is only as good as the data it is trained on. Given biased training data, BERT inevitably learns these biases. As an example, I fed the masked sentence "%3cmask> is a programmer" into BERT. Here are the top 10 predictions:%0a%0a- He%0a%0a- Daniel%0a%0a- Who%0a%0a- David%0a%0a- James%0a%0a- Craig%0a%0a- Smith%0a%0a- Cook%0a%0a- Simon%0a%0a- Williams%0a%0aThis, of course, is not BERT's fault; ultimately, it is the role of the researchers, data scientists, and programmers that create and use it to identify and eliminate these biases.%0a%0aReferences:%0a%0ahttps://www.aclweb.org/anthology/N19-1423/%0a%0ahttps://www.aclweb.org/anthology/2020.tacl-1.54/%0a%0ahttps://www.aclweb.org/anthology/2020.acl-main.383/%0a%0ahttps://arxiv.org/abs/1901.05287%0a%0a[[#contentends]]%0a----%0a----%0a%0a!!Expansions:%0a[[#expansions]]%0a%0a[[#expansionsend]]%0a%0a----%0a%25red%25 '''Add an expansion:'''%0a%0a(:if [ exists GradebookExpansions.{$Name}-{$Author} || equal {$Author} {$:Author} ] :)%0a%0a(:else:)%0a%0a(:foxform Site.FoxForms#newexpansion:)%0a%0a(:ifend:)%0a%0a----%0a----%0a%0a!!Comments%0a%0a%0a%0a(:section: B:)%0a(:Category: Varia:)
time=1620428647
author:1620428647=AlexX
diff:1620428647:1620428353:=37,74c37,59%0a%3c BERT, short for Bidirectional Encoder Representations from Transformers is a deep learning technique that has enabled massive advances in NLP tasks in recent years. Introduced in 2018 by Google researchers, BERT immediately yielded top results on tasks ranging from question answering to natural language understanding and inference; in the years following, it has been shown to push the state-of-the-art in many more tasks, including word sense disambiguation and text summarization. %0a%3c %0a%3c BERT uses the transformer neural network architecture, the details of which will be omitted in this article (though http://nlp.seas.harvard.edu/2018/04/03/attention.html has a very good description of it). What truly distinguishes BERT from past techniques, however, is the manner in which it is (pre-)trained. Prior to being applied to any task, BERT is pre-trained on two tasks. The first is the Cloze task (or masked language modelling), in which certain words in a sentence are hidden from the reader and must be guessed from context. For instance, given the sentence "I like %3cmask> on my hotdogs," BERT might learn to replace the mask with "ketchup" or "mustard." The second task is next sentence prediction, in which the reader is given two sentences and has to decide whether the second sentence can logically follow the first. While these two tasks may seem fairly trivial to us, they go a long way toward enhancing BERT's ability to model natural language.%0a%3c %0a%3c With that being said, researchers are still unsure as to why exactly BERT is so good at what it does. This has spawned an entire subfield of NLP known as "BERTology" dedicated to studying and explaining BERT. Interestingly, recent research has shown that BERT does have some concept of syntactic structure and consitutuency. For example, given the phrase "follow social media transitions on Capitol Hill," BERT's internal attentional mechanism learns to associate "transitions" more strongly with "media" than with "on," which reflects the fact that "media" and "transitions" are sisters in the syntax tree while "transitions" and "on" are not. Further, it has been shown that BERT "understands" agreement - for example, given a sentence like "The game %3cmask> bad," more often than not, BERT is able to correctly "is" rather than "are" to fill the mask.%0a%3c %0a%3c One caveat of all this is that as a language model, BERT is only as good as the data it is trained on. Given biased training data, BERT inevitably learns these biases. As an example, I fed the masked sentence "%3cmask> is a programmer" into BERT. Here are the top 10 predictions:%0a%3c %0a%3c - He%0a%3c %0a%3c - Daniel%0a%3c %0a%3c - Who%0a%3c %0a%3c - David%0a%3c %0a%3c - James%0a%3c %0a%3c - Craig%0a%3c %0a%3c - Smith%0a%3c %0a%3c - Cook%0a%3c %0a%3c - Simon%0a%3c %0a%3c - Williams%0a%3c %0a%3c This, of course, is not BERT's fault; ultimately, it is the role of the researchers, data scientists, and programmers that create and use it to identify and eliminate these biases.%0a%3c %0a%3c References:%0a%3c %0a%3c https://www.aclweb.org/anthology/N19-1423/%0a%3c %0a%3c https://www.aclweb.org/anthology/2020.tacl-1.54/%0a%3c %0a%3c https://www.aclweb.org/anthology/2020.acl-main.383/%0a%3c %0a---%0a> BERT, short for Bidirectional Encoder Representations from Transformers is a deep learning technique that has enabled massive advances in NLP tasks in recent years. Introduced in 2018 by Google researchers, BERT immediately yielded top results on tasks ranging from question answering to natural language understanding and inference; in the years following, it has been shown to push the state-of-the-art in many more tasks, including word sense disambiguation and text summarization. %0a> %0a> BERT uses the transformer neural network architecture, the details of which will be omitted in this article (though http://nlp.seas.harvard.edu/2018/04/03/attention.html has a very good description of it). What truly distinguishes BERT from past techniques, however, is the manner in which it is (pre-)trained. Prior to being applied to any task, BERT is pre-trained on two tasks. The first is the Cloze task (or masked language modelling), in which certain words in a sentence are hidden from the reader and must be guessed from context. For instance, given the sentence "I like %3cmask> on my hotdogs," BERT might learn to replace the mask with "ketchup" or "mustard." The second task is next sentence prediction, in which the reader is given two sentences and has to decide whether the second sentence can logically follow the first. While these two tasks may seem fairly trivial to us, they go a long way toward enhancing BERT's ability to model natural language.%0a> %0a> With that being said, researchers are still unsure as to why exactly BERT is so good at what it does. This has spawned an entire subfield of NLP known as "BERTology" dedicated to studying and explaining BERT. Interestingly, recent research has shown that BERT does have some concept of syntactic structure and consitutuency. For example, given the phrase "follow social media transitions on Capitol Hill," BERT's internal attentional mechanism learns to associate "transitions" more strongly with "media" than with "on," which reflects the fact that "media" and "transitions" are sisters in the syntax tree while "transitions" and "on" are not. Further, it has been shown that BERT "understands" agreement - for example, given a sentence like "The game %3cmask> bad," more often than not, BERT is able to correctly "is" rather than "are" to fill the mask.%0a> %0a> One caveat of all this is that as a language model, BERT is only as good as the data it is trained on. Given biased training data, BERT inevitably learns these biases. As an example, I fed the masked sentence "%3cmask> is a programmer" into BERT. Here are the top 10 predictions:%0a> - He%0a> - Daniel%0a> - Who%0a> - David%0a> - James%0a> - Craig%0a> - Smith%0a> - Cook%0a> - Simon%0a> - Williams%0a> This, of course, is not BERT's fault; ultimately, it is the role of the researchers, data scientists, and programmers that create and use it to identify and eliminate these biases.%0a> %0a> References:%0a> https://www.aclweb.org/anthology/N19-1423/%0a> https://www.aclweb.org/anthology/2020.tacl-1.54/%0a> https://www.aclweb.org/anthology/2020.acl-main.383/%0a
host:1620428647=71.206.238.88
author:1620428353=AlexX
diff:1620428353:1620428353:=1,90d0%0a%3c (:if authgroup @tas:)%0a%3c %0a%3c (:toggle id=gi show="show grading interface" hide="hide grading interface":)%0a%3c %0a%3c >>id=gi%3c%3c%0a%3c %0a%3c [[GradebookArticles.{$Name}|See article in gradebook]] \\%0a%3c [[{$FullName}?action=diff|See all changes to article]]%0a%3c %0a%3c (:foxform Site.FoxForms#gradeitem:)%0a%3c %0a%3c (:foxform Site.FoxForms#gradingcomment:)%0a%3c %0a%3c >>%3c%3c%0a%3c %0a%3c ----%0a%3c %0a%3c (:ifend:)%0a%3c %0a%3c [[!Varia]]%0a%3c %0a%3c !The Cult of BERTology%0a%3c %0a%3c :Author: AlexX%0a%3c %0a%3c '''Summary:''' \\%0a%3c %0a%3c [[#summary]]%0a%3c %0a%3c An introduction to BERT, the current state-of-the-art in natural language processing.%0a%3c %0a%3c [[#summaryends]]%0a%3c %0a%3c ----%0a%3c [[#content]]%0a%3c %0a%3c BERT, short for Bidirectional Encoder Representations from Transformers is a deep learning technique that has enabled massive advances in NLP tasks in recent years. Introduced in 2018 by Google researchers, BERT immediately yielded top results on tasks ranging from question answering to natural language understanding and inference; in the years following, it has been shown to push the state-of-the-art in many more tasks, including word sense disambiguation and text summarization. %0a%3c %0a%3c BERT uses the transformer neural network architecture, the details of which will be omitted in this article (though http://nlp.seas.harvard.edu/2018/04/03/attention.html has a very good description of it). What truly distinguishes BERT from past techniques, however, is the manner in which it is (pre-)trained. Prior to being applied to any task, BERT is pre-trained on two tasks. The first is the Cloze task (or masked language modelling), in which certain words in a sentence are hidden from the reader and must be guessed from context. For instance, given the sentence "I like %3cmask> on my hotdogs," BERT might learn to replace the mask with "ketchup" or "mustard." The second task is next sentence prediction, in which the reader is given two sentences and has to decide whether the second sentence can logically follow the first. While these two tasks may seem fairly trivial to us, they go a long way toward enhancing BERT's ability to model natural language.%0a%3c %0a%3c With that being said, researchers are still unsure as to why exactly BERT is so good at what it does. This has spawned an entire subfield of NLP known as "BERTology" dedicated to studying and explaining BERT. Interestingly, recent research has shown that BERT does have some concept of syntactic structure and consitutuency. For example, given the phrase "follow social media transitions on Capitol Hill," BERT's internal attentional mechanism learns to associate "transitions" more strongly with "media" than with "on," which reflects the fact that "media" and "transitions" are sisters in the syntax tree while "transitions" and "on" are not. Further, it has been shown that BERT "understands" agreement - for example, given a sentence like "The game %3cmask> bad," more often than not, BERT is able to correctly "is" rather than "are" to fill the mask.%0a%3c %0a%3c One caveat of all this is that as a language model, BERT is only as good as the data it is trained on. Given biased training data, BERT inevitably learns these biases. As an example, I fed the masked sentence "%3cmask> is a programmer" into BERT. Here are the top 10 predictions:%0a%3c - He%0a%3c - Daniel%0a%3c - Who%0a%3c - David%0a%3c - James%0a%3c - Craig%0a%3c - Smith%0a%3c - Cook%0a%3c - Simon%0a%3c - Williams%0a%3c This, of course, is not BERT's fault; ultimately, it is the role of the researchers, data scientists, and programmers that create and use it to identify and eliminate these biases.%0a%3c %0a%3c References:%0a%3c https://www.aclweb.org/anthology/N19-1423/%0a%3c https://www.aclweb.org/anthology/2020.tacl-1.54/%0a%3c https://www.aclweb.org/anthology/2020.acl-main.383/%0a%3c https://arxiv.org/abs/1901.05287%0a%3c %0a%3c [[#contentends]]%0a%3c ----%0a%3c ----%0a%3c %0a%3c !!Expansions:%0a%3c [[#expansions]]%0a%3c %0a%3c [[#expansionsend]]%0a%3c %0a%3c ----%0a%3c %25red%25 '''Add an expansion:'''%0a%3c %0a%3c (:if [ exists GradebookExpansions.{$Name}-{$Author} || equal {$Author} {$:Author} ] :)%0a%3c %0a%3c (:else:)%0a%3c %0a%3c (:foxform Site.FoxForms#newexpansion:)%0a%3c %0a%3c (:ifend:)%0a%3c %0a%3c ----%0a%3c ----%0a%3c %0a%3c !!Comments%0a%3c %0a%3c %0a%3c %0a%3c (:section: B:)%0a%3c (:Category: Varia:)%0a\ No newline at end of file%0a
host:1620428353=71.206.238.88
