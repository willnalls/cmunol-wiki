version=pmwiki-2.2.130 ordered=1 urlencoded=1
agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36
author=TA_lili
charset=UTF-8
csum=
ctime=1620427897
host=67.171.64.6
name=GradebookExpansions.BagOfWordsModelInNaturalLanguageProcessing-jjmonroe
rev=2
targets=Articles.BagOfWordsModelInNaturalLanguageProcessing,Profiles.Sebastiang
text=:Category: Varia%0a:Expansion: Use in natural language interpretation & user data analysis%0a:Author: jjmonroe%0a:Original: [[Articles.BagOfWordsModelInNaturalLanguageProcessing|BagOfWordsModelInNaturalLanguageProcessing]] %0a:OriginalAuthor: [[Profiles.sebastiang|sebastiang]]%0a:Section: C%0a:Completed: 07.05.2021 - 15:51%0a:Status: complete%0a%0a(:foxform Site.FoxForms#gradeExpansion:)%0a(:foxform Site.FoxForms#gradingcomment:)%0a%0a%0a%25red%25 '''Grading History'''%0a%0a[[#history]]%0a#foxbegin 210511-014144-320730#%0a* [-10.05.2021 - 18:41-] || TA_lili marked as complete%0a#foxend 210511-014144-320730#%0a[[#historyend]]%0a%0a%25red%25 '''Comments to student'''%0a%0a[[#comments]]%0a%0a[[#commentsend]]%0a%0a----%0a----%0a!!%0a%0a%0a----%0a[[#content]]%0a%0aIt's interesting to note that although Bag of Words isn't perfect, it's very viable as a "quick and dirty" solution when trying to find search results or related content. It turns out that trying to get a program to even approximate understanding an input is very hard, but you can get passable results in a lot of cases just by stripping away the noise words (identified in the post as determiners and prepositions) and comparing whatever is left. Going even lazier, you can use a binary Bag of Words in many cases, throwing out the frequency and just comparing present/absent.%0a%0aThis is clear in something like a search: if a user types in "How many miles away is the moon?", the list of results that contain a 1 to 1 verbatim match for that string will be limited. However, doing a very simple operation to convert it into the Bag of Words {"miles":1, "away":1, "moon":1} is much more likely to bring up relevant articles that have the information the person is looking for, without needing the question to be posed in exactly the same way.%0a%0aIs this better than a deep learning algorithm? Will it beat Watson? Obviously not. But it's much, much easier to implement than either of those two, and can deliver a /pretty good/ result more often than you might expect. It isn't perfect, but even a simplistic method like Bag of Words can often punch above its weight class.%0a%0a[[#contentends]]%0a----%0a(:Category:)%0a(:GradedBy: TA_lili:)
time=1620697303
author:1620697303=TA_lili
diff:1620697303:1620427897:=8,9c8,9%0a%3c :Status: complete%0a%3c %0a---%0a> :Status: ungraded%0a> %0a17,19c17%0a%3c #foxbegin 210511-014144-320730#%0a%3c * [-10.05.2021 - 18:41-] || TA_lili marked as complete%0a%3c #foxend 210511-014144-320730#%0a---%0a> %0a44,45c42%0a%3c (:Category:)%0a%3c (:GradedBy: TA_lili:)%0a\ No newline at end of file%0a---%0a> (:Category:)%0a\ No newline at end of file%0a
host:1620697303=67.171.64.6
author:1620427897=jjmonroe
diff:1620427897:1620427897:=1,42d0%0a%3c :Category: Varia%0a%3c :Expansion: Use in natural language interpretation & user data analysis%0a%3c :Author: jjmonroe%0a%3c :Original: [[Articles.BagOfWordsModelInNaturalLanguageProcessing|BagOfWordsModelInNaturalLanguageProcessing]] %0a%3c :OriginalAuthor: [[Profiles.sebastiang|sebastiang]]%0a%3c :Section: C%0a%3c :Completed: 07.05.2021 - 15:51%0a%3c :Status: ungraded%0a%3c %0a%3c (:foxform Site.FoxForms#gradeExpansion:)%0a%3c (:foxform Site.FoxForms#gradingcomment:)%0a%3c %0a%3c %0a%3c %25red%25 '''Grading History'''%0a%3c %0a%3c [[#history]]%0a%3c %0a%3c [[#historyend]]%0a%3c %0a%3c %25red%25 '''Comments to student'''%0a%3c %0a%3c [[#comments]]%0a%3c %0a%3c [[#commentsend]]%0a%3c %0a%3c ----%0a%3c ----%0a%3c !!%0a%3c %0a%3c %0a%3c ----%0a%3c [[#content]]%0a%3c %0a%3c It's interesting to note that although Bag of Words isn't perfect, it's very viable as a "quick and dirty" solution when trying to find search results or related content. It turns out that trying to get a program to even approximate understanding an input is very hard, but you can get passable results in a lot of cases just by stripping away the noise words (identified in the post as determiners and prepositions) and comparing whatever is left. Going even lazier, you can use a binary Bag of Words in many cases, throwing out the frequency and just comparing present/absent.%0a%3c %0a%3c This is clear in something like a search: if a user types in "How many miles away is the moon?", the list of results that contain a 1 to 1 verbatim match for that string will be limited. However, doing a very simple operation to convert it into the Bag of Words {"miles":1, "away":1, "moon":1} is much more likely to bring up relevant articles that have the information the person is looking for, without needing the question to be posed in exactly the same way.%0a%3c %0a%3c Is this better than a deep learning algorithm? Will it beat Watson? Obviously not. But it's much, much easier to implement than either of those two, and can deliver a /pretty good/ result more often than you might expect. It isn't perfect, but even a simplistic method like Bag of Words can often punch above its weight class.%0a%3c %0a%3c [[#contentends]]%0a%3c ----%0a%3c (:Category:)%0a\ No newline at end of file%0a
host:1620427897=73.174.7.241
