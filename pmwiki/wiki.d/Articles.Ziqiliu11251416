version=pmwiki-2.2.130 ordered=1 urlencoded=1
agent=Mozilla/5.0 (Windows NT 10.0; rv:106.0) Gecko/20100101 Firefox/106.0
author=nathandeyak
charset=UTF-8
csum=
ctime=1669419128
host=128.237.82.7
name=Articles.Ziqiliu11251416
rev=7
targets=GradebookArticles.Ziqiliu11251416,Articles.Ziqiliu11251416,Category.Varia
text=(:if [ authgroup @tas || authgroup @admins || equal {$Author} '' ]:)%0a%0a(:toggle id=gi show="show grading interface" hide="hide grading interface":)%0a%0a>>id=gi%3c%3c%0a%0a[[GradebookArticles.{$Name}|See article in gradebook]] \\%0a[[{$FullName}?action=diff|See all changes to article]] \\%0a[[{$FullName}?action=edit|Edit this page]] \\%0aStatus: {GradebookArticles.{$Name}$:Status} %0a%0a(:foxform Site.FoxForms#gradeitem:)%0a%0a(:foxform Site.FoxForms#gradingcomment:)%0a%0a>>%3c%3c%0a%0a----%0a%0a(:ifend:)%0a%0a[[!{$:Category}]]%0a%0a!Computational Linguistic: word vectors%0a%0a:Author: ziqiliu%0a%0a'''Summary:''' \\%0a%0a[[#summary]]%0a%0aHow does a computer know what "dog" means?%0a%0a[[#summaryends]]%0a%0a----%0a[[#content]]%0a%0a“Word vector” and Natural Language Processing (NLP) may sound remote to you, but you must have heard of machine translation (if not, try using Google translate for once).  It’s shocking to see how this algorithm can break down the language barrier with such ease, and it naturally begs the question of how such a feat is achieved. The biggest challenge of machine translation tasks is the computer representation of word meaning. How does a computer learn what a word means efficiently (feeding it the whole Oxford English Dictionary might work, but at what cost?). %0a%0aBefore the word vector comes out, the traditional go-to methods are what is called the “bag-of-words” method. As the name suggests, each sentence can be represented as a roster of some words of interest, with each slot meaning the presence or absence of a particular word. A shortcoming of this representation is its complete ignorance of the syntax of the sentence and the semantic varieties of each word, which are themselves essential constituents of “meaning.” %0a%0aHence comes the linguist John Rupert Firth who claimed in 1950: “You shall know a word by the company it keeps.” This is known as the distributional hypothesis, which argues that a word’s meaning is precisely the context it’s in. This heavily influenced the development of the Word2Vect algorithm 60 years later, which simply treats the “context” of a word as other words in a fixed range in its proximity (e.g., the preceding and proceeding five words). How does this algorithm work? Simple. Feed it a word. After a few milliseconds, the algorithm spits out a vector that supposedly encodes the approximate meaning of your input word. You might find it mystic and strange, but it’s actually quite intuitive. What this algorithm does behind the scene is project each word into a multi-dimensional geometric space, where words of similar meaning (i.e., sharing similar contexts) are close to each other (here, “close” takes a geometric interpretation).  Naturally, you might ask, what are those dimensions? For instance, the word “dog” might result in a four-dimensional vector, where the four dimension stands for the concept “animal,” “domesticated,” “pet,” and “fluffy,” and the numbers in each dimension represents how related the word “dog” is to each dimension word. In this way, the word “cat” will have a much more similar vector to “dog” than to "plane."  %0aI will end this article with the most famous magic trick performed by university professors to illustrate the success of this algorithm. If you run the algorithm on “king,” “man,” “woman,” and “queen,” you will see the following equation works out: %0a“King” - “man” + “woman” = “queen” (the equation is approximate). %0a%0aReference: %0ahttps://dzone.com/articles/introduction-to-word-vectors%0a%0a[[#contentends]]%0a----%0a----%0a%0a!!Expansions:%0a[[#expansions]]%0a#foxbegin 221209-230716-924360#%0a(:div1 class=expansionhead:)%0a!!!Word2Vec recently declared "dated" %0a-> - cassidy%0a-> [-09.12.2022 - 15:07-]%0a>>rfloat%3c%3c   %0a(:if1 authgroup @tas:)%0a{[foxdelrange button 221209-230716-924360 {$FullName} ]}%0a(:if1end:)%0a>>%3c%3c%0a(:div1end:) %0a>>messageitem%3c%3c %0aWord2Vec has recently -- in 2022 -- been described as dated. Transformer models are currently preferred.%0a%0aIf you've ever seen the GPT-2 or GPT-3 models, the latter of which has recently exploded in popularity, you've seen Transformer models in action in an NLP setting.%0a%0aTransformer models are a deep-learning neural net model that adapt to the context (i.e, of a word, so a similar idea of Word2Vec) using the linear algebra technique "attention" / "self-attention".One of the advantages of transformers is that datasets don't need to be labeled, which eliminates a lot of the human-involvement in managing AI training and allows larger corpuses (training datasets) to be used.%0a%0asources:%0a[1] https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9785808%0a%0a[2] https://openai.com/blog/better-language-models/%0a%0a[3] https://blogs.nvidia.com/blog/2022/03/25/what-is-a-transformer-model/%0a>>%3c%3c%0a#foxend 221209-230716-924360#%0a#foxbegin 221210-040542-44430#%0a(:div1 class=expansionhead:)%0a!!!Sentence vectorization %0a-> - nathandeyak%0a-> [-09.12.2022 - 20:05-]%0a>>rfloat%3c%3c   %0a(:if1 authgroup @tas:)%0a{[foxdelrange button 221210-040542-44430 {$FullName} ]}%0a(:if1end:)%0a>>%3c%3c%0a(:div1end:) %0a>>messageitem%3c%3c %0aOn a related subject, there is a similar way of modeling sentences quantitatively, where the meaning of a sentence is encoded in a vector, and this vector is intended to align with the vectors of similar sentences more closely than with different sentences. This is done also by using context, and training a model to pick a more natural sentence from an unnatural sentence.%0a%0ahttps://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html%0a>>%3c%3c%0a#foxend 221210-040542-44430#%0a[[#expansionsend]]%0a%0a----%0a%25red%25 '''Add an expansion:'''%0a%0a(:if [ exists GradebookExpansions.{$Name}-{$Author} || equal {$Author} {$:Author} ] :)%0a%0a(:elseif [ equal {Profiles.{$Author}$:andrewid} '' || equal {Profiles.{$Author}$:section} '' ] :)%0a%0aYou must enter your andrew ID and section before submitting an expansion. %0a%0a(:else:)%0a%0a(:foxform Site.FoxForms#newexpansion:)%0a%0a(:ifend:)%0a%0a----%0a----%0a%0a!!Comments%0a%0a%0a%0a%0a----%0a(:if equal {$Author} {$:Author}:)%0a!!Change article category%0a(:foxform Site.FoxForms#changearticlecategory:)%0a(:ifend:)%0a%0a%0a(:Section: C:)%0a(:Category: Varia:)%0a(:Title: Computational Linguistic: word vectors:)
time=1670645142
author:1670645142=nathandeyak
diff:1670645142:1670627236:=82,98d81%0a%3c #foxbegin 221210-040542-44430#%0a%3c (:div1 class=expansionhead:)%0a%3c !!!Sentence vectorization %0a%3c -> - nathandeyak%0a%3c -> [-09.12.2022 - 20:05-]%0a%3c >>rfloat%3c%3c   %0a%3c (:if1 authgroup @tas:)%0a%3c {[foxdelrange button 221210-040542-44430 {$FullName} ]}%0a%3c (:if1end:)%0a%3c >>%3c%3c%0a%3c (:div1end:) %0a%3c >>messageitem%3c%3c %0a%3c On a related subject, there is a similar way of modeling sentences quantitatively, where the meaning of a sentence is encoded in a vector, and this vector is intended to align with the vectors of similar sentences more closely than with different sentences. This is done also by using context, and training a model to pick a more natural sentence from an unnatural sentence.%0a%3c %0a%3c https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html%0a%3c >>%3c%3c%0a%3c #foxend 221210-040542-44430#%0a
host:1670645142=128.237.82.7
author:1670627236=cassidy
diff:1670627236:1670204597:=56,81d55%0a%3c #foxbegin 221209-230716-924360#%0a%3c (:div1 class=expansionhead:)%0a%3c !!!Word2Vec recently declared "dated" %0a%3c -> - cassidy%0a%3c -> [-09.12.2022 - 15:07-]%0a%3c >>rfloat%3c%3c   %0a%3c (:if1 authgroup @tas:)%0a%3c {[foxdelrange button 221209-230716-924360 {$FullName} ]}%0a%3c (:if1end:)%0a%3c >>%3c%3c%0a%3c (:div1end:) %0a%3c >>messageitem%3c%3c %0a%3c Word2Vec has recently -- in 2022 -- been described as dated. Transformer models are currently preferred.%0a%3c %0a%3c If you've ever seen the GPT-2 or GPT-3 models, the latter of which has recently exploded in popularity, you've seen Transformer models in action in an NLP setting.%0a%3c %0a%3c Transformer models are a deep-learning neural net model that adapt to the context (i.e, of a word, so a similar idea of Word2Vec) using the linear algebra technique "attention" / "self-attention".One of the advantages of transformers is that datasets don't need to be labeled, which eliminates a lot of the human-involvement in managing AI training and allows larger corpuses (training datasets) to be used.%0a%3c %0a%3c sources:%0a%3c [1] https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9785808%0a%3c %0a%3c [2] https://openai.com/blog/better-language-models/%0a%3c %0a%3c [3] https://blogs.nvidia.com/blog/2022/03/25/what-is-a-transformer-model/%0a%3c >>%3c%3c%0a%3c #foxend 221209-230716-924360#%0a
host:1670627236=128.237.82.18
author:1670204597=yuminf22ta
csum:1670204597=Post deleted
diff:1670204597:1670204573:=55a56,70%0a> #foxbegin 221203-015213-519800#%0a> (:div1 class=expansionhead:)%0a> !!! %0a> -> - piercemiller%0a> -> [-02.12.2022 - 17:52-]%0a> >>rfloat%3c%3c   %0a> (:if1 authgroup @tas:)%0a> {[foxdelrange button 221203-015213-519800 {$FullName} ]}%0a> (:if1end:)%0a> >>%3c%3c%0a> (:div1end:) %0a> >>messageitem%3c%3c %0a> %0a> >>%3c%3c%0a> #foxend 221203-015213-519800#%0a
host:1670204597=172.58.184.253
author:1670204573=yuminf22ta
csum:1670204573=Post deleted
diff:1670204573:1670032333:=55a56,70%0a> #foxbegin 221203-015203-854120#%0a> (:div1 class=expansionhead:)%0a> !!! %0a> -> - piercemiller%0a> -> [-02.12.2022 - 17:52-]%0a> >>rfloat%3c%3c   %0a> (:if1 authgroup @tas:)%0a> {[foxdelrange button 221203-015203-854120 {$FullName} ]}%0a> (:if1end:)%0a> >>%3c%3c%0a> (:div1end:) %0a> >>messageitem%3c%3c %0a> %0a> >>%3c%3c%0a> #foxend 221203-015203-854120#%0a
host:1670204573=172.58.184.253
author:1670032333=piercemiller
diff:1670032333:1670032322:=71,85d70%0a%3c #foxbegin 221203-015213-519800#%0a%3c (:div1 class=expansionhead:)%0a%3c !!! %0a%3c -> - piercemiller%0a%3c -> [-02.12.2022 - 17:52-]%0a%3c >>rfloat%3c%3c   %0a%3c (:if1 authgroup @tas:)%0a%3c {[foxdelrange button 221203-015213-519800 {$FullName} ]}%0a%3c (:if1end:)%0a%3c >>%3c%3c%0a%3c (:div1end:) %0a%3c >>messageitem%3c%3c %0a%3c %0a%3c >>%3c%3c%0a%3c #foxend 221203-015213-519800#%0a
host:1670032333=128.237.82.12
author:1670032322=piercemiller
diff:1670032322:1669419128:=56,70c56%0a%3c #foxbegin 221203-015203-854120#%0a%3c (:div1 class=expansionhead:)%0a%3c !!! %0a%3c -> - piercemiller%0a%3c -> [-02.12.2022 - 17:52-]%0a%3c >>rfloat%3c%3c   %0a%3c (:if1 authgroup @tas:)%0a%3c {[foxdelrange button 221203-015203-854120 {$FullName} ]}%0a%3c (:if1end:)%0a%3c >>%3c%3c%0a%3c (:div1end:) %0a%3c >>messageitem%3c%3c %0a%3c %0a%3c >>%3c%3c%0a%3c #foxend 221203-015203-854120#%0a---%0a> %0a
host:1670032322=128.237.82.12
author:1669419128=ziqiliu
diff:1669419128:1669419128:=1,91d0%0a%3c (:if [ authgroup @tas || authgroup @admins || equal {$Author} '' ]:)%0a%3c %0a%3c (:toggle id=gi show="show grading interface" hide="hide grading interface":)%0a%3c %0a%3c >>id=gi%3c%3c%0a%3c %0a%3c [[GradebookArticles.{$Name}|See article in gradebook]] \\%0a%3c [[{$FullName}?action=diff|See all changes to article]] \\%0a%3c [[{$FullName}?action=edit|Edit this page]] \\%0a%3c Status: {GradebookArticles.{$Name}$:Status} %0a%3c %0a%3c (:foxform Site.FoxForms#gradeitem:)%0a%3c %0a%3c (:foxform Site.FoxForms#gradingcomment:)%0a%3c %0a%3c >>%3c%3c%0a%3c %0a%3c ----%0a%3c %0a%3c (:ifend:)%0a%3c %0a%3c [[!{$:Category}]]%0a%3c %0a%3c !Computational Linguistic: word vectors%0a%3c %0a%3c :Author: ziqiliu%0a%3c %0a%3c '''Summary:''' \\%0a%3c %0a%3c [[#summary]]%0a%3c %0a%3c How does a computer know what "dog" means?%0a%3c %0a%3c [[#summaryends]]%0a%3c %0a%3c ----%0a%3c [[#content]]%0a%3c %0a%3c “Word vector” and Natural Language Processing (NLP) may sound remote to you, but you must have heard of machine translation (if not, try using Google translate for once).  It’s shocking to see how this algorithm can break down the language barrier with such ease, and it naturally begs the question of how such a feat is achieved. The biggest challenge of machine translation tasks is the computer representation of word meaning. How does a computer learn what a word means efficiently (feeding it the whole Oxford English Dictionary might work, but at what cost?). %0a%3c %0a%3c Before the word vector comes out, the traditional go-to methods are what is called the “bag-of-words” method. As the name suggests, each sentence can be represented as a roster of some words of interest, with each slot meaning the presence or absence of a particular word. A shortcoming of this representation is its complete ignorance of the syntax of the sentence and the semantic varieties of each word, which are themselves essential constituents of “meaning.” %0a%3c %0a%3c Hence comes the linguist John Rupert Firth who claimed in 1950: “You shall know a word by the company it keeps.” This is known as the distributional hypothesis, which argues that a word’s meaning is precisely the context it’s in. This heavily influenced the development of the Word2Vect algorithm 60 years later, which simply treats the “context” of a word as other words in a fixed range in its proximity (e.g., the preceding and proceeding five words). How does this algorithm work? Simple. Feed it a word. After a few milliseconds, the algorithm spits out a vector that supposedly encodes the approximate meaning of your input word. You might find it mystic and strange, but it’s actually quite intuitive. What this algorithm does behind the scene is project each word into a multi-dimensional geometric space, where words of similar meaning (i.e., sharing similar contexts) are close to each other (here, “close” takes a geometric interpretation).  Naturally, you might ask, what are those dimensions? For instance, the word “dog” might result in a four-dimensional vector, where the four dimension stands for the concept “animal,” “domesticated,” “pet,” and “fluffy,” and the numbers in each dimension represents how related the word “dog” is to each dimension word. In this way, the word “cat” will have a much more similar vector to “dog” than to "plane."  %0a%3c I will end this article with the most famous magic trick performed by university professors to illustrate the success of this algorithm. If you run the algorithm on “king,” “man,” “woman,” and “queen,” you will see the following equation works out: %0a%3c “King” - “man” + “woman” = “queen” (the equation is approximate). %0a%3c %0a%3c Reference: %0a%3c https://dzone.com/articles/introduction-to-word-vectors%0a%3c %0a%3c [[#contentends]]%0a%3c ----%0a%3c ----%0a%3c %0a%3c !!Expansions:%0a%3c [[#expansions]]%0a%3c %0a%3c [[#expansionsend]]%0a%3c %0a%3c ----%0a%3c %25red%25 '''Add an expansion:'''%0a%3c %0a%3c (:if [ exists GradebookExpansions.{$Name}-{$Author} || equal {$Author} {$:Author} ] :)%0a%3c %0a%3c (:elseif [ equal {Profiles.{$Author}$:andrewid} '' || equal {Profiles.{$Author}$:section} '' ] :)%0a%3c %0a%3c You must enter your andrew ID and section before submitting an expansion. %0a%3c %0a%3c (:else:)%0a%3c %0a%3c (:foxform Site.FoxForms#newexpansion:)%0a%3c %0a%3c (:ifend:)%0a%3c %0a%3c ----%0a%3c ----%0a%3c %0a%3c !!Comments%0a%3c %0a%3c %0a%3c %0a%3c %0a%3c ----%0a%3c (:if equal {$Author} {$:Author}:)%0a%3c !!Change article category%0a%3c (:foxform Site.FoxForms#changearticlecategory:)%0a%3c (:ifend:)%0a%3c %0a%3c %0a%3c (:Section: C:)%0a%3c (:Category: Varia:)%0a%3c (:Title: Computational Linguistic: word vectors:)%0a\ No newline at end of file%0a
host:1669419128=128.237.82.19
