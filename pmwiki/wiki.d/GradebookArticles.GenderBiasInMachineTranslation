version=pmwiki-2.2.130 ordered=1 urlencoded=1
agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36
author=TA_audrey
charset=UTF-8
csum=
ctime=1620441367
host=69.218.234.170
name=GradebookArticles.GenderBiasInMachineTranslation
rev=2
targets=
text=:Category: Varia%0a:Essential: {Category.Varia$:essential}%0a:Title: Gender Bias in Machine Translation%0a:Author: AlbertL%0a:Section: A%0a:Completed: 07.05.2021 - 19:36%0a:Status: complete%0a%0a(:foxform Site.FoxForms#gradeitem:)%0a(:foxform Site.FoxForms#gradingcomment:)%0a%0a----%0a%0a%0a%25blue%25 '''Grading History'''%0a%0a[[#history]]%0a#foxbegin 210511-234536-837820#%0a* [-11.05.2021 - 16:45-] || TA_audrey marked as complete%0a#foxend 210511-234536-837820#%0a[[#historyend]]%0a%0a%25blue%25 '''Comments for student'''%0a%0a[[#comments]]%0a%0a[[#commentsend]]%0a%0a----%0a----%0a!!%0a%0aSummary: \\%0a%0a[[#summary]]%0a%0aAutomatic Machine Translation between languages with different gender syntax reveals bias.%0a%0a[[#summaryends]]%0a%0a----%0a[[#content]]%0a%0ahttps://www.forbes.com/sites/parmyolson/2018/02/15/the-algorithm-that-helped-google-translate-become-sexist/?sh=34eb6e1e7daa%0a%0aTurkish does not have gendered pronouns while English does. Thus when translating from Turkish to English, a gender must be assigned to pronouns as they are translated. This reveals issues in translation AI as multiple translation services have seen real world gender biases introduced in these ambiguities. For example o bir muhendis in google translates to he is an engineer while o bir hemsire is translated to she is a nurse. In the ambiguity of differing syntaxes, the AI guesses based on what it has seen most often and as there are more texts talking about male engineers than female engineers due to the bias in the real world, the AI guesses male for engineers. Since its discovery the google team has made a note specifically addressing this sort of issue but it demonstrates how even translation AIs reflect implicit biases in their training data and differing syntaxes for source and target languages creates ambiguity for these biases to be shown.%0a%0a[[#contentends]]%0a----%0a(:GradedBy: TA_audrey:)
time=1620776736
author:1620776736=TA_audrey
diff:1620776736:1620441367:=7,8c7,8%0a%3c :Status: complete%0a%3c %0a---%0a> :Status: ungraded%0a> %0a18,20c18%0a%3c #foxbegin 210511-234536-837820#%0a%3c * [-11.05.2021 - 16:45-] || TA_audrey marked as complete%0a%3c #foxend 210511-234536-837820#%0a---%0a> %0a49,50c47%0a%3c ----%0a%3c (:GradedBy: TA_audrey:)%0a\ No newline at end of file%0a---%0a> ----%0a\ No newline at end of file%0a
host:1620776736=69.218.234.170
author:1620441367=AlbertL
diff:1620441367:1620441367:=1,47d0%0a%3c :Category: Varia%0a%3c :Essential: {Category.Varia$:essential}%0a%3c :Title: Gender Bias in Machine Translation%0a%3c :Author: AlbertL%0a%3c :Section: A%0a%3c :Completed: 07.05.2021 - 19:36%0a%3c :Status: ungraded%0a%3c %0a%3c (:foxform Site.FoxForms#gradeitem:)%0a%3c (:foxform Site.FoxForms#gradingcomment:)%0a%3c %0a%3c ----%0a%3c %0a%3c %0a%3c %25blue%25 '''Grading History'''%0a%3c %0a%3c [[#history]]%0a%3c %0a%3c [[#historyend]]%0a%3c %0a%3c %25blue%25 '''Comments for student'''%0a%3c %0a%3c [[#comments]]%0a%3c %0a%3c [[#commentsend]]%0a%3c %0a%3c ----%0a%3c ----%0a%3c !!%0a%3c %0a%3c Summary: \\%0a%3c %0a%3c [[#summary]]%0a%3c %0a%3c Automatic Machine Translation between languages with different gender syntax reveals bias.%0a%3c %0a%3c [[#summaryends]]%0a%3c %0a%3c ----%0a%3c [[#content]]%0a%3c %0a%3c https://www.forbes.com/sites/parmyolson/2018/02/15/the-algorithm-that-helped-google-translate-become-sexist/?sh=34eb6e1e7daa%0a%3c %0a%3c Turkish does not have gendered pronouns while English does. Thus when translating from Turkish to English, a gender must be assigned to pronouns as they are translated. This reveals issues in translation AI as multiple translation services have seen real world gender biases introduced in these ambiguities. For example o bir muhendis in google translates to he is an engineer while o bir hemsire is translated to she is a nurse. In the ambiguity of differing syntaxes, the AI guesses based on what it has seen most often and as there are more texts talking about male engineers than female engineers due to the bias in the real world, the AI guesses male for engineers. Since its discovery the google team has made a note specifically addressing this sort of issue but it demonstrates how even translation AIs reflect implicit biases in their training data and differing syntaxes for source and target languages creates ambiguity for these biases to be shown.%0a%3c %0a%3c [[#contentends]]%0a%3c ----%0a\ No newline at end of file%0a
host:1620441367=67.163.151.95
