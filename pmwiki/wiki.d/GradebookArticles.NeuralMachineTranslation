version=pmwiki-2.2.130 ordered=1 urlencoded=1
agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.85 Safari/537.36
author=TA_audrey
charset=UTF-8
csum=
ctime=1618789392
host=69.218.234.170
name=GradebookArticles.NeuralMachineTranslation
rev=2
targets=
text=:Category: Varia%0a:Essential: {Category.Varia$:essential}%0a:Title: Neural Machine Translation%0a:Author: aryans%0a:Section: B%0a:Completed: 18.04.2021 - 16:43%0a:Status: complete%0a%0a(:foxform Site.FoxForms#gradeitem:)%0a(:foxform Site.FoxForms#gradingcomment:)%0a%0a----%0a%0a%0a%25blue%25 '''Grading History'''%0a%0a[[#history]]%0a#foxbegin 210504-193004-992590#%0a* [-04.05.2021 - 12:30-] || TA_audrey marked as complete%0a#foxend 210504-193004-992590#%0a[[#historyend]]%0a%0a%25blue%25 '''Comments for student'''%0a%0a[[#comments]]%0a%0a[[#commentsend]]%0a%0a----%0a----%0a!!%0a%0aSummary: \\%0a%0a[[#summary]]%0a%0aAn overview of neural machine translation (NMT) and a summary of research analyzing the parts of pretrained NMT models %0a%0a[[#summaryends]]%0a%0a----%0a[[#content]]%0a%0aMachine translation (MT) is a subfield of computational linguistics that seeks to create algorithms that translate from one language to another. Today, we will see a change that has taken place in MT research in recent years, a problem that this change poses, and how some researchers have attempted to address it.  %0a%0aIn the past, MT has usually been done using separate components that each construct one piece of the puzzle. For example, one algorithm might directly translate the words, another might apply appropriate affixes to the translated words to create agreement in the translated language, yet another might reorder the words appropriately, and so on.  Usually, these sub-algorithms use statistical methods relying on examples of already translated text. After they are created, the sub-algorithms are concatenated to create the final MT algorithm.%0a%0aHowever, researchers have recently shifted to using deep neural networks for the MT task--this is called neural machine translation (NMT). Today's NMT models, instead of dividing translation into multiple steps, are trained end-to-end. This means that they use a single deep neural net to do the entire translation, usually sentence by sentence, and that this neural net is trained all together, rather than different parts of it being trained as sub-algorithms. %0a%0aWhile this method is promising, a notable downside of NMT is that researchers are less sure about which pieces of the translation process are being done where in the NMT algorithm. Information like this provides valuable feedback on how to improve the algorithms. Therefore, a team of researchers out of MIT and Qatar Computing Research Institute have analyzed the representations of the data at various points in the NMT to ascertain whether they contain any linguistic information that common sense would suggest, such as information about the morphology, syntax, and semantics of the data being translated. The researchers concluded that morphological information is usually learned at lower layers, earlier in the neural net, while syntax and semantics are learned at higher layers, later in the algorithm.  %0a%0aAnother thing the researchers looked at is how the structure of the input data affects the efficacy of the final algorithm. Here, they concluded that representing the data as individual characters usually produced models better at identifying unfamiliar words, and that this method should be used "when translating morphologically rich languages such as Czech" (41). However, using subword units, rather than individual characters, was better for translation between "syntactically divergent language pairs such as German-English". This is because if two languages have notably different syntax, translation relies more on understanding the relationship between two words that could be far apart in one language but close together when you translate into the other, and this is somehow harder to capture when the input is passed in character by character. %0a%0aSources: %0a%0a- Belinkov et al. "On the Linguistic Representation Power of Neural Machine Translation Models" (2019)%0a%0a- Wikipedia: %0ahttps://en.wikipedia.org/wiki/Machine_translation%0ahttps://en.wikipedia.org/wiki/Neural_machine_translation%0a%0a%0a[[#contentends]]%0a----%0a(:GradedBy: TA_audrey:)
time=1620156604
author:1620156604=TA_audrey
diff:1620156604:1618789392:=7,8c7,8%0a%3c :Status: complete%0a%3c %0a---%0a> :Status: ungraded%0a> %0a18,20c18%0a%3c #foxbegin 210504-193004-992590#%0a%3c * [-04.05.2021 - 12:30-] || TA_audrey marked as complete%0a%3c #foxend 210504-193004-992590#%0a---%0a> %0a64,65c62%0a%3c ----%0a%3c (:GradedBy: TA_audrey:)%0a\ No newline at end of file%0a---%0a> ----%0a\ No newline at end of file%0a
host:1620156604=69.218.234.170
author:1618789392=aryans
diff:1618789392:1618789392:=1,62d0%0a%3c :Category: Varia%0a%3c :Essential: {Category.Varia$:essential}%0a%3c :Title: Neural Machine Translation%0a%3c :Author: aryans%0a%3c :Section: B%0a%3c :Completed: 18.04.2021 - 16:43%0a%3c :Status: ungraded%0a%3c %0a%3c (:foxform Site.FoxForms#gradeitem:)%0a%3c (:foxform Site.FoxForms#gradingcomment:)%0a%3c %0a%3c ----%0a%3c %0a%3c %0a%3c %25blue%25 '''Grading History'''%0a%3c %0a%3c [[#history]]%0a%3c %0a%3c [[#historyend]]%0a%3c %0a%3c %25blue%25 '''Comments for student'''%0a%3c %0a%3c [[#comments]]%0a%3c %0a%3c [[#commentsend]]%0a%3c %0a%3c ----%0a%3c ----%0a%3c !!%0a%3c %0a%3c Summary: \\%0a%3c %0a%3c [[#summary]]%0a%3c %0a%3c An overview of neural machine translation (NMT) and a summary of research analyzing the parts of pretrained NMT models %0a%3c %0a%3c [[#summaryends]]%0a%3c %0a%3c ----%0a%3c [[#content]]%0a%3c %0a%3c Machine translation (MT) is a subfield of computational linguistics that seeks to create algorithms that translate from one language to another. Today, we will see a change that has taken place in MT research in recent years, a problem that this change poses, and how some researchers have attempted to address it.  %0a%3c %0a%3c In the past, MT has usually been done using separate components that each construct one piece of the puzzle. For example, one algorithm might directly translate the words, another might apply appropriate affixes to the translated words to create agreement in the translated language, yet another might reorder the words appropriately, and so on.  Usually, these sub-algorithms use statistical methods relying on examples of already translated text. After they are created, the sub-algorithms are concatenated to create the final MT algorithm.%0a%3c %0a%3c However, researchers have recently shifted to using deep neural networks for the MT task--this is called neural machine translation (NMT). Today's NMT models, instead of dividing translation into multiple steps, are trained end-to-end. This means that they use a single deep neural net to do the entire translation, usually sentence by sentence, and that this neural net is trained all together, rather than different parts of it being trained as sub-algorithms. %0a%3c %0a%3c While this method is promising, a notable downside of NMT is that researchers are less sure about which pieces of the translation process are being done where in the NMT algorithm. Information like this provides valuable feedback on how to improve the algorithms. Therefore, a team of researchers out of MIT and Qatar Computing Research Institute have analyzed the representations of the data at various points in the NMT to ascertain whether they contain any linguistic information that common sense would suggest, such as information about the morphology, syntax, and semantics of the data being translated. The researchers concluded that morphological information is usually learned at lower layers, earlier in the neural net, while syntax and semantics are learned at higher layers, later in the algorithm.  %0a%3c %0a%3c Another thing the researchers looked at is how the structure of the input data affects the efficacy of the final algorithm. Here, they concluded that representing the data as individual characters usually produced models better at identifying unfamiliar words, and that this method should be used "when translating morphologically rich languages such as Czech" (41). However, using subword units, rather than individual characters, was better for translation between "syntactically divergent language pairs such as German-English". This is because if two languages have notably different syntax, translation relies more on understanding the relationship between two words that could be far apart in one language but close together when you translate into the other, and this is somehow harder to capture when the input is passed in character by character. %0a%3c %0a%3c Sources: %0a%3c %0a%3c - Belinkov et al. "On the Linguistic Representation Power of Neural Machine Translation Models" (2019)%0a%3c %0a%3c - Wikipedia: %0a%3c https://en.wikipedia.org/wiki/Machine_translation%0a%3c https://en.wikipedia.org/wiki/Neural_machine_translation%0a%3c %0a%3c %0a%3c [[#contentends]]%0a%3c ----%0a\ No newline at end of file%0a
host:1618789392=184.190.142.79
