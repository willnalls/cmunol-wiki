version=pmwiki-2.2.130 ordered=1 urlencoded=1
agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36
author=Russell_E
charset=UTF-8
csum=
ctime=1619914459
host=99.171.140.109
name=Articles.BagOfWordsModelInNaturalLanguageProcessing
rev=7
targets=GradebookArticles.BagOfWordsModelInNaturalLanguageProcessing,Articles.BagOfWordsModelInNaturalLanguageProcessing,Category.Varia
text=(:if authgroup @tas:)%0a%0a(:toggle id=gi show="show grading interface" hide="hide grading interface":)%0a%0a>>id=gi%3c%3c%0a%0a[[GradebookArticles.{$Name}|See article in gradebook]] \\%0a[[{$FullName}?action=diff|See all changes to article]]%0a%0a(:foxform Site.FoxForms#gradeitem:)%0a%0a(:foxform Site.FoxForms#gradingcomment:)%0a%0a>>%3c%3c%0a%0a----%0a%0a(:ifend:)%0a%0a[[!Varia]]%0a%0a!Bag of Words Model in Natural Language Processing%0a%0a:Author: sebastiang%0a%0a'''Summary:''' \\%0a%0a[[#summary]]%0a%0aDescription and Uses of the Bag of Words Model%0a%0a[[#summaryends]]%0a%0a----%0a[[#content]]%0a%0aThe Bag of Words model is a data structure that stores a set of words that appear in a text and their corresponding frequencies. Below is an example of how a sentence is transformed into a Bag of Words (BoW). %0a%0a(1) John likes to watch movies. Mary likes movies too.%0a%0aBoW1 = {"John":1,"likes":2,"to":1,"watch":1,"movies":2,"Mary":1,"too":1};%0a%0aSince all information about word order and grammar is lost when words are added to the Bag, its uses are limited. %0a%0aHowever, frequency on its own can still be useful in some cases. For example, the Bag of Words model is often used for spam filtering. This is accomplished by putting specific words from thousands of spam emails into a Bag of Words. Then the spam filter simply checks if words in the email are also in the Bag.%0a%0aOne challenge of this is example is knowing which words should be put in the bag. For example; prepositions and determiners are not good indicators of spam. %0a%0aAnother version of the Bag of Words is called the bigram model that stores sets of two consecutive words. An example is shown below.%0a%0a[%0a    "John likes",%0a    "likes to",%0a    "to watch",%0a    "watch movies",%0a    "Mary likes",%0a    "likes movies",%0a    "movies too",%0a]%0a%0aThis model can be more useful because it stores the syntax of a text to a degree. For example, from the bigram example of above we can see that "likes" can take a prepositional phrase complement headed by "to". Also, "likes" can take a CNP complement "movies". %0a%0aWith regards to the spam filtering example, a bigram model can be more useful than a Bag of Words because it is more sensitive to the grammar used in spam emails. In some cases, spam emails are in a language the spammer is not fluent in, so he will use improper grammar. A Bag of Words model may not detect this difference, but a bigram model may.%0a%0aSource: https://en.wikipedia.org/wiki/Bag-of-words_model%0a%0a%0a%0a%0a%0a[[#contentends]]%0a----%0a----%0a%0a!!Expansions:%0a[[#expansions]]%0a#foxbegin 210503-064625-106940#%0a(:div1 class=expansionhead:)%0a!!! %0a-> - NickG%0a(:div1end:) %0a>>messageitem%3c%3c %0aThe bigram model is really interesting and can even be extended into an n-gram model for arbitrary n. In fact, different n-grams can operate vastly differently depending  on how a language is formed. For instance, it is found that n-grams for lesser values of n work well for languages whose words carry a lot of grammatical information. Spanish, for example, sees better results from a bigram then English in a lot of cases because it takes, on average, fewer words to form the same Spanish sentence from an English sentence. Conversely, trigram models tend to work better for English. Moreover, these models can actually work extremely well in some cases as to differentiate between speakers of the same language. Given a large enough dataset of text from a certain speaker, an n-gram model can check some new text against the various frequencies it has computed to give some probability that the new text corresponds to that speaker.%0a%0aSource: %0ahttps://web.stanford.edu/~jurafsky/slp3/3.pdf%0ahttps://towardsdatascience.com/introduction-to-language-models-n-gram-e323081503d9#:~:text=The%2520Bigram%2520Model,%253A%2520P(the%2520%257C%2520that)%0a>>%3c%3c%0a#foxend 210503-064625-106940#%0a#foxbegin 210505-035408-56380#%0a(:div1 class=expansionhead:)%0a!!!Sentiment Analysis %0a-> - caseywalker%0a(:div1end:) %0a>>messageitem%3c%3c %0aAnother application for something like this is sentiment analysis, where text is categorized by sentiment, or put into classes based on some training data, etc. It's actually my group's Numerical Methods topic for our semester project. We used what is basically a bag of words and implemented a Naïve Bayes model to try and categorize movie reviews as positive or negative based on their word frequencies. We came up with ~80%25 for finding positive reviews and ~90%25 for finding negative reviews using this.%0a%0aAn interesting (and presumably very challenging/computationally expensive) method that compounds on the bag of words model is lemmatizing (and/or stemming) words, so that words with the same meaning such as 'likes' and 'liked' would be considered the same word. I think I recall reading that it usually has minimal impact on predictions, however, so it's not super useful.%0a%0aOne last thing: you mentioned preposition words and such -- those are (and many more) are called 'stop' words and are generally removed from bag of words models because they add little to the meaning of sentences.%0a%0asources:%0a-https://sebastianraschka.com/Articles/2014_naive_bayes_1.html#n-grams%0a>>%3c%3c%0a#foxend 210505-035408-56380#%0a#foxbegin 210506-030558-95850#%0a(:div1 class=expansionhead:)%0a!!!Scam Detection %0a-> - MindeeL%0a(:div1end:) %0a>>messageitem%3c%3c %0aYou mentioned how the bag of words model can be used to detect certain words that are commonly used in scams. Because of this, when scammers use misspellings or replace certain letters with symbols it often helps avoid detection. As a side note that I thought was interesting, another reason why scammers may do this is because people who fall for  scams which seem obviously fake because of bad grammar and spelling are likely more gullible targets for scammers to get money and information off of. %0aSource - https://www.priteshpawar.com/why-do-spammers-misspell-words/cybersecurity/priteshpawar/%0a>>%3c%3c%0a#foxend 210506-030558-95850#%0a#foxbegin 210507-225138-733310#%0a(:div1 class=expansionhead:)%0a!!!Use in natural language interpretation & user data analysis %0a-> - jjmonroe%0a(:div1end:) %0a>>messageitem%3c%3c %0aIt's interesting to note that although Bag of Words isn't perfect, it's very viable as a "quick and dirty" solution when trying to find search results or related content. It turns out that trying to get a program to even approximate understanding an input is very hard, but you can get passable results in a lot of cases just by stripping away the noise words (identified in the post as determiners and prepositions) and comparing whatever is left. Going even lazier, you can use a binary Bag of Words in many cases, throwing out the frequency and just comparing present/absent.%0a%0aThis is clear in something like a search: if a user types in "How many miles away is the moon?", the list of results that contain a 1 to 1 verbatim match for that string will be limited. However, doing a very simple operation to convert it into the Bag of Words {"miles":1, "away":1, "moon":1} is much more likely to bring up relevant articles that have the information the person is looking for, without needing the question to be posed in exactly the same way.%0a%0aIs this better than a deep learning algorithm? Will it beat Watson? Obviously not. But it's much, much easier to implement than either of those two, and can deliver a /pretty good/ result more often than you might expect. It isn't perfect, but even a simplistic method like Bag of Words can often punch above its weight class.%0a>>%3c%3c%0a#foxend 210507-225138-733310#%0a#foxbegin 210509-005520-363880#%0a(:div1 class=expansionhead:)%0a!!!Byte Pair Encoding %0a-> - Russell_E%0a(:div1end:) %0a>>messageitem%3c%3c %0aNick mentions a generalization of the bigram model to an n-gram model. One way to further generalize this is a concept called byte pair encoding. This model is built from the bottom up. A fun application of BPE is shown in this video:%0ahttps://www.youtube.com/watch?v=_ry6S-Dc2X8%0a%0aIn the video, BPE is explained in terms of characters. The model starts with individual characters as interesting strings. Then, it finds the most common combination of two interesting strings and adds it as a new interesting string. This can be repeated however many times as desired. %0a%0aBPE was originally created as a form of data compression. More interesting strings makes the original message shorter when encoded, but storing all the interesting strings eventually outweighs it if there are too many. However, it also has very useful applications in natural language processing when it's mixed with machine learning. When based on characters, BPE not only can recognize words, but can also make complex morphological analysis. Changing the number of interesting strings qualitatively changes analysis. If BPE is applied with words as the starting units, could it recognize high-level syntactic patterns? It's an interesting possibility.%0a%0aIt's also explained here:%0ahttps://en.wikipedia.org/wiki/Byte_pair_encoding%0a>>%3c%3c%0a#foxend 210509-005520-363880#%0a[[#expansionsend]]%0a%0a----%0a%25red%25 '''Add an expansion:'''%0a%0a(:if [ exists GradebookExpansions.{$Name}-{$Author} || equal {$Author} {$:Author} ] :)%0a%0a(:else:)%0a%0a(:foxform Site.FoxForms#newexpansion:)%0a%0a(:ifend:)%0a%0a----%0a----%0a%0a!!Comments%0a#foxbegin 210507-023107-978340#%0a(:div1 class=messagehead:)%0a>>rfloat%3c%3c   %0a[-06.05.2021 - 19:31-] &nbsp; %0a>>%3c%3c%0a(:if1 authgroup @tas:)%0a>>rfloat%3c%3c%0a{[foxdelrange button 210507-023107-978340 {$FullName} ]}%0a>>%3c%3c%0a(:if1end:)%0a!!!!!AlexanderW%0a(:div1end:) %0a>>messageitem%3c%3c %0a'''GPT3'''%0a>>messageitem%3c%3c%0ahttps://www.mosaicdatascience.com/2020/08/25/language-model-review-gpt3/#:~:text=In%2520the%2520early%2520days%2520of,into%2520the%2520same%2520%25E2%2580%259Cbag%25E2%2580%259D.%0a%0aAs the link above describes, the current word-generating AI's still use basic concepts from bag-of-words. They have since improved the model, which tries to encompass more general grammatical behavior, as well as the relatedness between words. %0a>>%3c%3c%0a#foxend 210507-023107-978340#%0a%0a%0a%0a(:section: F:)%0a(:Category: Varia:)
time=1620521720
author:1620521720=Russell_E
diff:1620521720:1620427897:=129,145d128%0a%3c #foxbegin 210509-005520-363880#%0a%3c (:div1 class=expansionhead:)%0a%3c !!!Byte Pair Encoding %0a%3c -> - Russell_E%0a%3c (:div1end:) %0a%3c >>messageitem%3c%3c %0a%3c Nick mentions a generalization of the bigram model to an n-gram model. One way to further generalize this is a concept called byte pair encoding. This model is built from the bottom up. A fun application of BPE is shown in this video:%0a%3c https://www.youtube.com/watch?v=_ry6S-Dc2X8%0a%3c %0a%3c In the video, BPE is explained in terms of characters. The model starts with individual characters as interesting strings. Then, it finds the most common combination of two interesting strings and adds it as a new interesting string. This can be repeated however many times as desired. %0a%3c %0a%3c BPE was originally created as a form of data compression. More interesting strings makes the original message shorter when encoded, but storing all the interesting strings eventually outweighs it if there are too many. However, it also has very useful applications in natural language processing when it's mixed with machine learning. When based on characters, BPE not only can recognize words, but can also make complex morphological analysis. Changing the number of interesting strings qualitatively changes analysis. If BPE is applied with words as the starting units, could it recognize high-level syntactic patterns? It's an interesting possibility.%0a%3c %0a%3c It's also explained here:%0a%3c https://en.wikipedia.org/wiki/Byte_pair_encoding%0a%3c >>%3c%3c%0a%3c #foxend 210509-005520-363880#%0a
host:1620521720=99.171.140.109
author:1620427897=jjmonroe
diff:1620427897:1620354667:=116,128d115%0a%3c #foxbegin 210507-225138-733310#%0a%3c (:div1 class=expansionhead:)%0a%3c !!!Use in natural language interpretation & user data analysis %0a%3c -> - jjmonroe%0a%3c (:div1end:) %0a%3c >>messageitem%3c%3c %0a%3c It's interesting to note that although Bag of Words isn't perfect, it's very viable as a "quick and dirty" solution when trying to find search results or related content. It turns out that trying to get a program to even approximate understanding an input is very hard, but you can get passable results in a lot of cases just by stripping away the noise words (identified in the post as determiners and prepositions) and comparing whatever is left. Going even lazier, you can use a binary Bag of Words in many cases, throwing out the frequency and just comparing present/absent.%0a%3c %0a%3c This is clear in something like a search: if a user types in "How many miles away is the moon?", the list of results that contain a 1 to 1 verbatim match for that string will be limited. However, doing a very simple operation to convert it into the Bag of Words {"miles":1, "away":1, "moon":1} is much more likely to bring up relevant articles that have the information the person is looking for, without needing the question to be posed in exactly the same way.%0a%3c %0a%3c Is this better than a deep learning algorithm? Will it beat Watson? Obviously not. But it's much, much easier to implement than either of those two, and can deliver a /pretty good/ result more often than you might expect. It isn't perfect, but even a simplistic method like Bag of Words can often punch above its weight class.%0a%3c >>%3c%3c%0a%3c #foxend 210507-225138-733310#%0a
host:1620427897=73.174.7.241
author:1620354667=AlexanderW
diff:1620354667:1620270357:=133,152d132%0a%3c #foxbegin 210507-023107-978340#%0a%3c (:div1 class=messagehead:)%0a%3c >>rfloat%3c%3c   %0a%3c [-06.05.2021 - 19:31-] &nbsp; %0a%3c >>%3c%3c%0a%3c (:if1 authgroup @tas:)%0a%3c >>rfloat%3c%3c%0a%3c {[foxdelrange button 210507-023107-978340 {$FullName} ]}%0a%3c >>%3c%3c%0a%3c (:if1end:)%0a%3c !!!!!AlexanderW%0a%3c (:div1end:) %0a%3c >>messageitem%3c%3c %0a%3c '''GPT3'''%0a%3c >>messageitem%3c%3c%0a%3c https://www.mosaicdatascience.com/2020/08/25/language-model-review-gpt3/#:~:text=In%2520the%2520early%2520days%2520of,into%2520the%2520same%2520%25E2%2580%259Cbag%25E2%2580%259D.%0a%3c %0a%3c As the link above describes, the current word-generating AI's still use basic concepts from bag-of-words. They have since improved the model, which tries to encompass more general grammatical behavior, as well as the relatedness between words. %0a%3c >>%3c%3c%0a%3c #foxend 210507-023107-978340#%0a
host:1620354667=73.90.192.150
author:1620270357=MindeeL
diff:1620270357:1620186847:=106,115d105%0a%3c #foxbegin 210506-030558-95850#%0a%3c (:div1 class=expansionhead:)%0a%3c !!!Scam Detection %0a%3c -> - MindeeL%0a%3c (:div1end:) %0a%3c >>messageitem%3c%3c %0a%3c You mentioned how the bag of words model can be used to detect certain words that are commonly used in scams. Because of this, when scammers use misspellings or replace certain letters with symbols it often helps avoid detection. As a side note that I thought was interesting, another reason why scammers may do this is because people who fall for  scams which seem obviously fake because of bad grammar and spelling are likely more gullible targets for scammers to get money and information off of. %0a%3c Source - https://www.priteshpawar.com/why-do-spammers-misspell-words/cybersecurity/priteshpawar/%0a%3c >>%3c%3c%0a%3c #foxend 210506-030558-95850#%0a
host:1620270357=72.95.138.86
author:1620186847=caseywalker
diff:1620186847:1620024385:=90,105d89%0a%3c #foxbegin 210505-035408-56380#%0a%3c (:div1 class=expansionhead:)%0a%3c !!!Sentiment Analysis %0a%3c -> - caseywalker%0a%3c (:div1end:) %0a%3c >>messageitem%3c%3c %0a%3c Another application for something like this is sentiment analysis, where text is categorized by sentiment, or put into classes based on some training data, etc. It's actually my group's Numerical Methods topic for our semester project. We used what is basically a bag of words and implemented a Naïve Bayes model to try and categorize movie reviews as positive or negative based on their word frequencies. We came up with ~80%25 for finding positive reviews and ~90%25 for finding negative reviews using this.%0a%3c %0a%3c An interesting (and presumably very challenging/computationally expensive) method that compounds on the bag of words model is lemmatizing (and/or stemming) words, so that words with the same meaning such as 'likes' and 'liked' would be considered the same word. I think I recall reading that it usually has minimal impact on predictions, however, so it's not super useful.%0a%3c %0a%3c One last thing: you mentioned preposition words and such -- those are (and many more) are called 'stop' words and are generally removed from bag of words models because they add little to the meaning of sentences.%0a%3c %0a%3c sources:%0a%3c -https://sebastianraschka.com/Articles/2014_naive_bayes_1.html#n-grams%0a%3c >>%3c%3c%0a%3c #foxend 210505-035408-56380#%0a
host:1620186847=74.109.239.4
author:1620024385=NickG
diff:1620024385:1619914459:=77,89c77%0a%3c #foxbegin 210503-064625-106940#%0a%3c (:div1 class=expansionhead:)%0a%3c !!! %0a%3c -> - NickG%0a%3c (:div1end:) %0a%3c >>messageitem%3c%3c %0a%3c The bigram model is really interesting and can even be extended into an n-gram model for arbitrary n. In fact, different n-grams can operate vastly differently depending  on how a language is formed. For instance, it is found that n-grams for lesser values of n work well for languages whose words carry a lot of grammatical information. Spanish, for example, sees better results from a bigram then English in a lot of cases because it takes, on average, fewer words to form the same Spanish sentence from an English sentence. Conversely, trigram models tend to work better for English. Moreover, these models can actually work extremely well in some cases as to differentiate between speakers of the same language. Given a large enough dataset of text from a certain speaker, an n-gram model can check some new text against the various frequencies it has computed to give some probability that the new text corresponds to that speaker.%0a%3c %0a%3c Source: %0a%3c https://web.stanford.edu/~jurafsky/slp3/3.pdf%0a%3c https://towardsdatascience.com/introduction-to-language-models-n-gram-e323081503d9#:~:text=The%2520Bigram%2520Model,%253A%2520P(the%2520%257C%2520that)%0a%3c >>%3c%3c%0a%3c #foxend 210503-064625-106940#%0a---%0a> %0a
host:1620024385=74.109.251.161
author:1619914459=sebastiang
diff:1619914459:1619914459:=1,99d0%0a%3c (:if authgroup @tas:)%0a%3c %0a%3c (:toggle id=gi show="show grading interface" hide="hide grading interface":)%0a%3c %0a%3c >>id=gi%3c%3c%0a%3c %0a%3c [[GradebookArticles.{$Name}|See article in gradebook]] \\%0a%3c [[{$FullName}?action=diff|See all changes to article]]%0a%3c %0a%3c (:foxform Site.FoxForms#gradeitem:)%0a%3c %0a%3c (:foxform Site.FoxForms#gradingcomment:)%0a%3c %0a%3c >>%3c%3c%0a%3c %0a%3c ----%0a%3c %0a%3c (:ifend:)%0a%3c %0a%3c [[!Varia]]%0a%3c %0a%3c !Bag of Words Model in Natural Language Processing%0a%3c %0a%3c :Author: sebastiang%0a%3c %0a%3c '''Summary:''' \\%0a%3c %0a%3c [[#summary]]%0a%3c %0a%3c Description and Uses of the Bag of Words Model%0a%3c %0a%3c [[#summaryends]]%0a%3c %0a%3c ----%0a%3c [[#content]]%0a%3c %0a%3c The Bag of Words model is a data structure that stores a set of words that appear in a text and their corresponding frequencies. Below is an example of how a sentence is transformed into a Bag of Words (BoW). %0a%3c %0a%3c (1) John likes to watch movies. Mary likes movies too.%0a%3c %0a%3c BoW1 = {"John":1,"likes":2,"to":1,"watch":1,"movies":2,"Mary":1,"too":1};%0a%3c %0a%3c Since all information about word order and grammar is lost when words are added to the Bag, its uses are limited. %0a%3c %0a%3c However, frequency on its own can still be useful in some cases. For example, the Bag of Words model is often used for spam filtering. This is accomplished by putting specific words from thousands of spam emails into a Bag of Words. Then the spam filter simply checks if words in the email are also in the Bag.%0a%3c %0a%3c One challenge of this is example is knowing which words should be put in the bag. For example; prepositions and determiners are not good indicators of spam. %0a%3c %0a%3c Another version of the Bag of Words is called the bigram model that stores sets of two consecutive words. An example is shown below.%0a%3c %0a%3c [%0a%3c     "John likes",%0a%3c     "likes to",%0a%3c     "to watch",%0a%3c     "watch movies",%0a%3c     "Mary likes",%0a%3c     "likes movies",%0a%3c     "movies too",%0a%3c ]%0a%3c %0a%3c This model can be more useful because it stores the syntax of a text to a degree. For example, from the bigram example of above we can see that "likes" can take a prepositional phrase complement headed by "to". Also, "likes" can take a CNP complement "movies". %0a%3c %0a%3c With regards to the spam filtering example, a bigram model can be more useful than a Bag of Words because it is more sensitive to the grammar used in spam emails. In some cases, spam emails are in a language the spammer is not fluent in, so he will use improper grammar. A Bag of Words model may not detect this difference, but a bigram model may.%0a%3c %0a%3c Source: https://en.wikipedia.org/wiki/Bag-of-words_model%0a%3c %0a%3c %0a%3c %0a%3c %0a%3c %0a%3c [[#contentends]]%0a%3c ----%0a%3c ----%0a%3c %0a%3c !!Expansions:%0a%3c [[#expansions]]%0a%3c %0a%3c [[#expansionsend]]%0a%3c %0a%3c ----%0a%3c %25red%25 '''Add an expansion:'''%0a%3c %0a%3c (:if [ exists GradebookExpansions.{$Name}-{$Author} || equal {$Author} {$:Author} ] :)%0a%3c %0a%3c (:else:)%0a%3c %0a%3c (:foxform Site.FoxForms#newexpansion:)%0a%3c %0a%3c (:ifend:)%0a%3c %0a%3c ----%0a%3c ----%0a%3c %0a%3c !!Comments%0a%3c %0a%3c %0a%3c %0a%3c (:section: F:)%0a%3c (:Category: Varia:)%0a\ No newline at end of file%0a
host:1619914459=128.237.82.2
