version=pmwiki-2.2.130 ordered=1 urlencoded=1
agent=Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:84.0) Gecko/20100101 Firefox/84.0
author=
charset=UTF-8
csum=
ctime=1620449577
host=74.109.239.200
name=GradebookExpansions.GenderBiasInMachineTranslation-AlexX
rev=2
targets=Articles.GenderBiasInMachineTranslation,Profiles.AlbertL
text=:Category: Varia%0a:Expansion: %0a:Author: AlexX%0a:Original: [[Articles.GenderBiasInMachineTranslation|GenderBiasInMachineTranslation]] %0a:OriginalAuthor: [[Profiles.AlbertL|AlbertL]]%0a:Section: B%0a:Completed: 07.05.2021 - 21:52%0a:Status: ungraded%0a%0a(:foxform Site.FoxForms#gradeExpansion:)%0a(:foxform Site.FoxForms#gradingcomment:)%0a%0a%0a%25red%25 '''Grading History'''%0a%0a[[#history]]%0a%0a[[#historyend]]%0a%0a%25red%25 '''Comments to student'''%0a%0a[[#comments]]%0a%0a[[#commentsend]]%0a%0a----%0a----%0a!!%0a%0a%0a----%0a[[#content]]%0a%0aGiven the vast amount of people that use machine translation services like Google Translate on a daily basis, gender bias (as well as bias of any sort) is without question a very serious issue; if left unchecked, these tools might well end up serving to confirm and reinforce harmful stereotypes and biases. Fortunately, researchers seem to be aware of this problem and are working on solutions to it. One solution that has been proposed is to de-bias models by stripping their internal word representations of gender associations - this, however, deprives models of useful information and diminishes performance. Another approach has been to provide extra information to models by explicitly stating the gender of each sentence that is translated. However, this information may not be available in practice, which may force models to make assumptions about gender which may serve only to add more gender bias. A more promising proposal has been to eschew the male/female dichotomy and instead use solely non-gendered/non-binary language, even for grammatically gendered languages like Spanish. This approach ensures that minimal assumptions are made about the gender of the speaker. Most importantly, however, is the diversity of the individuals responsible for building these machine translation systems, from translators to annotators to data scientists and programmers.%0a%0a%0aReferences:%0a%0ahttps://arxiv.org/pdf/2104.06001.pdf%0a%0ahttps://towardsdatascience.com/gender-bias-in-machine-translation-819ddce2c452%0a%0a[[#contentends]]%0a----%0a(:Category:)%0a#foxbegin 210512-000935-760260#%0a!!Backup interface%0a(:foxform Site.FoxForms#gradeExpansion:)%0a#foxend 210512-000935-760260#
time=1620778172
author:1620778172=
diff:1620778172:1620449577:=45,49c45%0a%3c (:Category:)%0a%3c #foxbegin 210512-000935-760260#%0a%3c !!Backup interface%0a%3c (:foxform Site.FoxForms#gradeExpansion:)%0a%3c #foxend 210512-000935-760260#%0a\ No newline at end of file%0a---%0a> (:Category:)%0a\ No newline at end of file%0a
host:1620778172=74.109.239.200
author:1620449577=AlexX
diff:1620449577:1620449577:=1,45d0%0a%3c :Category: Varia%0a%3c :Expansion: %0a%3c :Author: AlexX%0a%3c :Original: [[Articles.GenderBiasInMachineTranslation|GenderBiasInMachineTranslation]] %0a%3c :OriginalAuthor: [[Profiles.AlbertL|AlbertL]]%0a%3c :Section: B%0a%3c :Completed: 07.05.2021 - 21:52%0a%3c :Status: ungraded%0a%3c %0a%3c (:foxform Site.FoxForms#gradeExpansion:)%0a%3c (:foxform Site.FoxForms#gradingcomment:)%0a%3c %0a%3c %0a%3c %25red%25 '''Grading History'''%0a%3c %0a%3c [[#history]]%0a%3c %0a%3c [[#historyend]]%0a%3c %0a%3c %25red%25 '''Comments to student'''%0a%3c %0a%3c [[#comments]]%0a%3c %0a%3c [[#commentsend]]%0a%3c %0a%3c ----%0a%3c ----%0a%3c !!%0a%3c %0a%3c %0a%3c ----%0a%3c [[#content]]%0a%3c %0a%3c Given the vast amount of people that use machine translation services like Google Translate on a daily basis, gender bias (as well as bias of any sort) is without question a very serious issue; if left unchecked, these tools might well end up serving to confirm and reinforce harmful stereotypes and biases. Fortunately, researchers seem to be aware of this problem and are working on solutions to it. One solution that has been proposed is to de-bias models by stripping their internal word representations of gender associations - this, however, deprives models of useful information and diminishes performance. Another approach has been to provide extra information to models by explicitly stating the gender of each sentence that is translated. However, this information may not be available in practice, which may force models to make assumptions about gender which may serve only to add more gender bias. A more promising proposal has been to eschew the male/female dichotomy and instead use solely non-gendered/non-binary language, even for grammatically gendered languages like Spanish. This approach ensures that minimal assumptions are made about the gender of the speaker. Most importantly, however, is the diversity of the individuals responsible for building these machine translation systems, from translators to annotators to data scientists and programmers.%0a%3c %0a%3c %0a%3c References:%0a%3c %0a%3c https://arxiv.org/pdf/2104.06001.pdf%0a%3c %0a%3c https://towardsdatascience.com/gender-bias-in-machine-translation-819ddce2c452%0a%3c %0a%3c [[#contentends]]%0a%3c ----%0a%3c (:Category:)%0a\ No newline at end of file%0a
host:1620449577=71.206.238.88
