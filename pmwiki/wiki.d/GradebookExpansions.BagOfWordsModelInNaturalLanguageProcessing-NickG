version=pmwiki-2.2.130 ordered=1 urlencoded=1
agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36
author=TA_lili
charset=UTF-8
csum=
ctime=1620024385
host=74.111.97.246
name=GradebookExpansions.BagOfWordsModelInNaturalLanguageProcessing-NickG
rev=2
targets=Articles.BagOfWordsModelInNaturalLanguageProcessing,Profiles.Sebastiang
text=:Category: Varia%0a:Expansion: %0a:Author: NickG%0a:Original: [[Articles.BagOfWordsModelInNaturalLanguageProcessing|BagOfWordsModelInNaturalLanguageProcessing]] %0a:OriginalAuthor: [[Profiles.sebastiang|sebastiang]]%0a:Section: C%0a:Completed: 02.05.2021 - 23:46%0a:Status: complete%0a%0a(:foxform Site.FoxForms#gradeExpansion:)%0a(:foxform Site.FoxForms#gradingcomment:)%0a%0a%0a%25red%25 '''Grading History'''%0a%0a[[#history]]%0a#foxbegin 210506-152853-602130#%0a* [-06.05.2021 - 08:28-] || TA_lili marked as complete%0a#foxend 210506-152853-602130#%0a[[#historyend]]%0a%0a%25red%25 '''Comments to student'''%0a%0a[[#comments]]%0a%0a[[#commentsend]]%0a%0a----%0a----%0a!!%0a%0a%0a----%0a[[#content]]%0a%0aThe bigram model is really interesting and can even be extended into an n-gram model for arbitrary n. In fact, different n-grams can operate vastly differently depending  on how a language is formed. For instance, it is found that n-grams for lesser values of n work well for languages whose words carry a lot of grammatical information. Spanish, for example, sees better results from a bigram then English in a lot of cases because it takes, on average, fewer words to form the same Spanish sentence from an English sentence. Conversely, trigram models tend to work better for English. Moreover, these models can actually work extremely well in some cases as to differentiate between speakers of the same language. Given a large enough dataset of text from a certain speaker, an n-gram model can check some new text against the various frequencies it has computed to give some probability that the new text corresponds to that speaker.%0a%0aSource: %0ahttps://web.stanford.edu/~jurafsky/slp3/3.pdf%0ahttps://towardsdatascience.com/introduction-to-language-models-n-gram-e323081503d9#:~:text=The%2520Bigram%2520Model,%253A%2520P(the%2520%257C%2520that)%0a%0a[[#contentends]]%0a----%0a(:Category:)%0a(:GradedBy: TA_lili:)
time=1620314933
author:1620314933=TA_lili
diff:1620314933:1620024385:=8,9c8,9%0a%3c :Status: complete%0a%3c %0a---%0a> :Status: ungraded%0a> %0a17,19c17%0a%3c #foxbegin 210506-152853-602130#%0a%3c * [-06.05.2021 - 08:28-] || TA_lili marked as complete%0a%3c #foxend 210506-152853-602130#%0a---%0a> %0a44,45c42%0a%3c (:Category:)%0a%3c (:GradedBy: TA_lili:)%0a\ No newline at end of file%0a---%0a> (:Category:)%0a\ No newline at end of file%0a
host:1620314933=74.111.97.246
author:1620024385=NickG
diff:1620024385:1620024385:=1,42d0%0a%3c :Category: Varia%0a%3c :Expansion: %0a%3c :Author: NickG%0a%3c :Original: [[Articles.BagOfWordsModelInNaturalLanguageProcessing|BagOfWordsModelInNaturalLanguageProcessing]] %0a%3c :OriginalAuthor: [[Profiles.sebastiang|sebastiang]]%0a%3c :Section: C%0a%3c :Completed: 02.05.2021 - 23:46%0a%3c :Status: ungraded%0a%3c %0a%3c (:foxform Site.FoxForms#gradeExpansion:)%0a%3c (:foxform Site.FoxForms#gradingcomment:)%0a%3c %0a%3c %0a%3c %25red%25 '''Grading History'''%0a%3c %0a%3c [[#history]]%0a%3c %0a%3c [[#historyend]]%0a%3c %0a%3c %25red%25 '''Comments to student'''%0a%3c %0a%3c [[#comments]]%0a%3c %0a%3c [[#commentsend]]%0a%3c %0a%3c ----%0a%3c ----%0a%3c !!%0a%3c %0a%3c %0a%3c ----%0a%3c [[#content]]%0a%3c %0a%3c The bigram model is really interesting and can even be extended into an n-gram model for arbitrary n. In fact, different n-grams can operate vastly differently depending  on how a language is formed. For instance, it is found that n-grams for lesser values of n work well for languages whose words carry a lot of grammatical information. Spanish, for example, sees better results from a bigram then English in a lot of cases because it takes, on average, fewer words to form the same Spanish sentence from an English sentence. Conversely, trigram models tend to work better for English. Moreover, these models can actually work extremely well in some cases as to differentiate between speakers of the same language. Given a large enough dataset of text from a certain speaker, an n-gram model can check some new text against the various frequencies it has computed to give some probability that the new text corresponds to that speaker.%0a%3c %0a%3c Source: %0a%3c https://web.stanford.edu/~jurafsky/slp3/3.pdf%0a%3c https://towardsdatascience.com/introduction-to-language-models-n-gram-e323081503d9#:~:text=The%2520Bigram%2520Model,%253A%2520P(the%2520%257C%2520that)%0a%3c %0a%3c [[#contentends]]%0a%3c ----%0a%3c (:Category:)%0a\ No newline at end of file%0a
host:1620024385=74.109.251.161
