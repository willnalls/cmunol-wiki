version=pmwiki-2.2.130 ordered=1 urlencoded=1
agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36
author=saumyaB
charset=UTF-8
csum=
ctime=1620441367
host=128.237.82.1
name=Articles.GenderBiasInMachineTranslation
rev=3
targets=GradebookArticles.GenderBiasInMachineTranslation,Articles.GenderBiasInMachineTranslation,Category.Varia
text=(:if authgroup @tas:)%0a%0a(:toggle id=gi show="show grading interface" hide="hide grading interface":)%0a%0a>>id=gi%3c%3c%0a%0a[[GradebookArticles.{$Name}|See article in gradebook]] \\%0a[[{$FullName}?action=diff|See all changes to article]]%0a%0a(:foxform Site.FoxForms#gradeitem:)%0a%0a(:foxform Site.FoxForms#gradingcomment:)%0a%0a>>%3c%3c%0a%0a----%0a%0a(:ifend:)%0a%0a[[!Varia]]%0a%0a!Gender Bias in Machine Translation%0a%0a:Author: AlbertL%0a%0a'''Summary:''' \\%0a%0a[[#summary]]%0a%0aAutomatic Machine Translation between languages with different gender syntax reveals bias.%0a%0a[[#summaryends]]%0a%0a----%0a[[#content]]%0a%0ahttps://www.forbes.com/sites/parmyolson/2018/02/15/the-algorithm-that-helped-google-translate-become-sexist/?sh=34eb6e1e7daa%0a%0aTurkish does not have gendered pronouns while English does. Thus when translating from Turkish to English, a gender must be assigned to pronouns as they are translated. This reveals issues in translation AI as multiple translation services have seen real world gender biases introduced in these ambiguities. For example o bir muhendis in google translates to he is an engineer while o bir hemsire is translated to she is a nurse. In the ambiguity of differing syntaxes, the AI guesses based on what it has seen most often and as there are more texts talking about male engineers than female engineers due to the bias in the real world, the AI guesses male for engineers. Since its discovery the google team has made a note specifically addressing this sort of issue but it demonstrates how even translation AIs reflect implicit biases in their training data and differing syntaxes for source and target languages creates ambiguity for these biases to be shown.%0a%0a[[#contentends]]%0a----%0a----%0a%0a!!Expansions:%0a[[#expansions]]%0a#foxbegin 210508-045257-668590#%0a(:div1 class=expansionhead:)%0a!!! %0a-> - AlexX%0a(:div1end:) %0a>>messageitem%3c%3c %0aGiven the vast amount of people that use machine translation services like Google Translate on a daily basis, gender bias (as well as bias of any sort) is without question a very serious issue; if left unchecked, these tools might well end up serving to confirm and reinforce harmful stereotypes and biases. Fortunately, researchers seem to be aware of this problem and are working on solutions to it. One solution that has been proposed is to de-bias models by stripping their internal word representations of gender associations - this, however, deprives models of useful information and diminishes performance. Another approach has been to provide extra information to models by explicitly stating the gender of each sentence that is translated. However, this information may not be available in practice, which may force models to make assumptions about gender which may serve only to add more gender bias. A more promising proposal has been to eschew the male/female dichotomy and instead use solely non-gendered/non-binary language, even for grammatically gendered languages like Spanish. This approach ensures that minimal assumptions are made about the gender of the speaker. Most importantly, however, is the diversity of the individuals responsible for building these machine translation systems, from translators to annotators to data scientists and programmers.%0a%0a%0aReferences:%0a%0ahttps://arxiv.org/pdf/2104.06001.pdf%0a%0ahttps://towardsdatascience.com/gender-bias-in-machine-translation-819ddce2c452%0a>>%3c%3c%0a#foxend 210508-045257-668590#%0a[[#expansionsend]]%0a%0a----%0a%25red%25 '''Add an expansion:'''%0a%0a(:if [ exists GradebookExpansions.{$Name}-{$Author} || equal {$Author} {$:Author} ] :)%0a%0a(:else:)%0a%0a(:foxform Site.FoxForms#newexpansion:)%0a%0a(:ifend:)%0a%0a----%0a----%0a%0a!!Comments%0a#foxbegin 210513-204105-527780#%0a(:div1 class=messagehead:)%0a>>rfloat%3c%3c   %0a[-13.05.2021 - 13:41-] &nbsp; %0a>>%3c%3c%0a(:if1 authgroup @tas:)%0a>>rfloat%3c%3c%0a{[foxdelrange button 210513-204105-527780 {$FullName} ]}%0a>>%3c%3c%0a(:if1end:)%0a!!!!!saumyaB%0a(:div1end:) %0a>>messageitem%3c%3c %0a'''Fixing bias in technology'''%0a>>messageitem%3c%3c%0aI wonder what the solution to this would be. I've seen people pushing for more variety in datasets used in machine learning and more diverse people on teams developing this technology. I hope we can move toward these solutions so in the future these biases are removed. %0a>>%3c%3c%0a#foxend 210513-204105-527780#%0a%0a%0a%0a(:section: A:)%0a(:Category: Varia:)
time=1620938464
author:1620938464=saumyaB
diff:1620938464:1620449577:=80,97d79%0a%3c #foxbegin 210513-204105-527780#%0a%3c (:div1 class=messagehead:)%0a%3c >>rfloat%3c%3c   %0a%3c [-13.05.2021 - 13:41-] &nbsp; %0a%3c >>%3c%3c%0a%3c (:if1 authgroup @tas:)%0a%3c >>rfloat%3c%3c%0a%3c {[foxdelrange button 210513-204105-527780 {$FullName} ]}%0a%3c >>%3c%3c%0a%3c (:if1end:)%0a%3c !!!!!saumyaB%0a%3c (:div1end:) %0a%3c >>messageitem%3c%3c %0a%3c '''Fixing bias in technology'''%0a%3c >>messageitem%3c%3c%0a%3c I wonder what the solution to this would be. I've seen people pushing for more variety in datasets used in machine learning and more diverse people on teams developing this technology. I hope we can move toward these solutions so in the future these biases are removed. %0a%3c >>%3c%3c%0a%3c #foxend 210513-204105-527780#%0a
host:1620938464=128.237.82.1
author:1620449577=AlexX
diff:1620449577:1620441367:=47,62c47%0a%3c #foxbegin 210508-045257-668590#%0a%3c (:div1 class=expansionhead:)%0a%3c !!! %0a%3c -> - AlexX%0a%3c (:div1end:) %0a%3c >>messageitem%3c%3c %0a%3c Given the vast amount of people that use machine translation services like Google Translate on a daily basis, gender bias (as well as bias of any sort) is without question a very serious issue; if left unchecked, these tools might well end up serving to confirm and reinforce harmful stereotypes and biases. Fortunately, researchers seem to be aware of this problem and are working on solutions to it. One solution that has been proposed is to de-bias models by stripping their internal word representations of gender associations - this, however, deprives models of useful information and diminishes performance. Another approach has been to provide extra information to models by explicitly stating the gender of each sentence that is translated. However, this information may not be available in practice, which may force models to make assumptions about gender which may serve only to add more gender bias. A more promising proposal has been to eschew the male/female dichotomy and instead use solely non-gendered/non-binary language, even for grammatically gendered languages like Spanish. This approach ensures that minimal assumptions are made about the gender of the speaker. Most importantly, however, is the diversity of the individuals responsible for building these machine translation systems, from translators to annotators to data scientists and programmers.%0a%3c %0a%3c %0a%3c References:%0a%3c %0a%3c https://arxiv.org/pdf/2104.06001.pdf%0a%3c %0a%3c https://towardsdatascience.com/gender-bias-in-machine-translation-819ddce2c452%0a%3c >>%3c%3c%0a%3c #foxend 210508-045257-668590#%0a---%0a> %0a
host:1620449577=71.206.238.88
author:1620441367=AlbertL
diff:1620441367:1620441367:=1,69d0%0a%3c (:if authgroup @tas:)%0a%3c %0a%3c (:toggle id=gi show="show grading interface" hide="hide grading interface":)%0a%3c %0a%3c >>id=gi%3c%3c%0a%3c %0a%3c [[GradebookArticles.{$Name}|See article in gradebook]] \\%0a%3c [[{$FullName}?action=diff|See all changes to article]]%0a%3c %0a%3c (:foxform Site.FoxForms#gradeitem:)%0a%3c %0a%3c (:foxform Site.FoxForms#gradingcomment:)%0a%3c %0a%3c >>%3c%3c%0a%3c %0a%3c ----%0a%3c %0a%3c (:ifend:)%0a%3c %0a%3c [[!Varia]]%0a%3c %0a%3c !Gender Bias in Machine Translation%0a%3c %0a%3c :Author: AlbertL%0a%3c %0a%3c '''Summary:''' \\%0a%3c %0a%3c [[#summary]]%0a%3c %0a%3c Automatic Machine Translation between languages with different gender syntax reveals bias.%0a%3c %0a%3c [[#summaryends]]%0a%3c %0a%3c ----%0a%3c [[#content]]%0a%3c %0a%3c https://www.forbes.com/sites/parmyolson/2018/02/15/the-algorithm-that-helped-google-translate-become-sexist/?sh=34eb6e1e7daa%0a%3c %0a%3c Turkish does not have gendered pronouns while English does. Thus when translating from Turkish to English, a gender must be assigned to pronouns as they are translated. This reveals issues in translation AI as multiple translation services have seen real world gender biases introduced in these ambiguities. For example o bir muhendis in google translates to he is an engineer while o bir hemsire is translated to she is a nurse. In the ambiguity of differing syntaxes, the AI guesses based on what it has seen most often and as there are more texts talking about male engineers than female engineers due to the bias in the real world, the AI guesses male for engineers. Since its discovery the google team has made a note specifically addressing this sort of issue but it demonstrates how even translation AIs reflect implicit biases in their training data and differing syntaxes for source and target languages creates ambiguity for these biases to be shown.%0a%3c %0a%3c [[#contentends]]%0a%3c ----%0a%3c ----%0a%3c %0a%3c !!Expansions:%0a%3c [[#expansions]]%0a%3c %0a%3c [[#expansionsend]]%0a%3c %0a%3c ----%0a%3c %25red%25 '''Add an expansion:'''%0a%3c %0a%3c (:if [ exists GradebookExpansions.{$Name}-{$Author} || equal {$Author} {$:Author} ] :)%0a%3c %0a%3c (:else:)%0a%3c %0a%3c (:foxform Site.FoxForms#newexpansion:)%0a%3c %0a%3c (:ifend:)%0a%3c %0a%3c ----%0a%3c ----%0a%3c %0a%3c !!Comments%0a%3c %0a%3c %0a%3c %0a%3c (:section: A:)%0a%3c (:Category: Varia:)%0a\ No newline at end of file%0a
host:1620441367=67.163.151.95
