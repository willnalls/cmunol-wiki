version=pmwiki-2.2.130 ordered=1 urlencoded=1
agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36
author=TA_lili
charset=UTF-8
csum=
ctime=1620443740
host=67.171.64.6
name=GradebookExpansions.SignLanguageInVirtualReality-CharlieP
rev=2
targets=Articles.SignLanguageInVirtualReality,Profiles.Kevinx
text=:Category: SignLanguage%0a:Expansion: Computer Vision in Hand Tracking for Sign Language%0a:Author: CharlieP%0a:Original: [[Articles.SignLanguageInVirtualReality|SignLanguageInVirtualReality]] %0a:OriginalAuthor: [[Profiles.kevinx|kevinx]]%0a:Section: D%0a:Completed: 07.05.2021 - 20:15%0a:Status: complete%0a%0a(:foxform Site.FoxForms#gradeExpansion:)%0a(:foxform Site.FoxForms#gradingcomment:)%0a%0a%0a%25red%25 '''Grading History'''%0a%0a[[#history]]%0a#foxbegin 210511-024310-258580#%0a* [-10.05.2021 - 19:43-] || TA_lili marked as complete%0a#foxend 210511-024310-258580#%0a[[#historyend]]%0a%0a%25red%25 '''Comments to student'''%0a%0a[[#comments]]%0a%0a[[#commentsend]]%0a%0a----%0a----%0a!!%0a%0a%0a----%0a[[#content]]%0a%0aThe current technology of sign language in VR is through tracking of the hands as rigid bodies, that is, that the individual motion of the fingers or arms isn't tracked, only the position and rotation of the hand in space, as well as whether the controller is "gripped" or "open" is tracked. Something that could make sign language in VR much more natural is tracking of the hands and arms using computer vision. This method would track the individual fingers as well as the arms, and possibly the face of the speaker, to better convey more sign language in VR, as well as lower the learning curve for new users. I think that the reason finger tracking in VR isn't more popular is that it's not necessary for playing games, and typically you would be rotating in your room, not just facing one direction. Typically, you don't rotate to various positions when you're having a conversation in real life, so this solution may be viable. %0a%0aHere's a blog post illustrating some of Google's hand recognition technology: %0ahttps://ai.googleblog.com/2019/08/on-device-real-time-hand-tracking-with.html%0a%0aSources: %0ahttps://towardsdatascience.com/sign-language-recognition-using-deep-learning-6549268c60bd%0ahttps://paperswithcode.com/task/sign-language-recognition%0a%0a%0a[[#contentends]]%0a----%0a(:Category:)%0a(:GradedBy: TA_lili:)
time=1620700990
author:1620700990=TA_lili
diff:1620700990:1620443740:=8,9c8,9%0a%3c :Status: complete%0a%3c %0a---%0a> :Status: ungraded%0a> %0a17,19c17%0a%3c #foxbegin 210511-024310-258580#%0a%3c * [-10.05.2021 - 19:43-] || TA_lili marked as complete%0a%3c #foxend 210511-024310-258580#%0a---%0a> %0a48,49c46%0a%3c (:Category:)%0a%3c (:GradedBy: TA_lili:)%0a\ No newline at end of file%0a---%0a> (:Category:)%0a\ No newline at end of file%0a
host:1620700990=67.171.64.6
author:1620443740=CharlieP
diff:1620443740:1620443740:=1,46d0%0a%3c :Category: SignLanguage%0a%3c :Expansion: Computer Vision in Hand Tracking for Sign Language%0a%3c :Author: CharlieP%0a%3c :Original: [[Articles.SignLanguageInVirtualReality|SignLanguageInVirtualReality]] %0a%3c :OriginalAuthor: [[Profiles.kevinx|kevinx]]%0a%3c :Section: D%0a%3c :Completed: 07.05.2021 - 20:15%0a%3c :Status: ungraded%0a%3c %0a%3c (:foxform Site.FoxForms#gradeExpansion:)%0a%3c (:foxform Site.FoxForms#gradingcomment:)%0a%3c %0a%3c %0a%3c %25red%25 '''Grading History'''%0a%3c %0a%3c [[#history]]%0a%3c %0a%3c [[#historyend]]%0a%3c %0a%3c %25red%25 '''Comments to student'''%0a%3c %0a%3c [[#comments]]%0a%3c %0a%3c [[#commentsend]]%0a%3c %0a%3c ----%0a%3c ----%0a%3c !!%0a%3c %0a%3c %0a%3c ----%0a%3c [[#content]]%0a%3c %0a%3c The current technology of sign language in VR is through tracking of the hands as rigid bodies, that is, that the individual motion of the fingers or arms isn't tracked, only the position and rotation of the hand in space, as well as whether the controller is "gripped" or "open" is tracked. Something that could make sign language in VR much more natural is tracking of the hands and arms using computer vision. This method would track the individual fingers as well as the arms, and possibly the face of the speaker, to better convey more sign language in VR, as well as lower the learning curve for new users. I think that the reason finger tracking in VR isn't more popular is that it's not necessary for playing games, and typically you would be rotating in your room, not just facing one direction. Typically, you don't rotate to various positions when you're having a conversation in real life, so this solution may be viable. %0a%3c %0a%3c Here's a blog post illustrating some of Google's hand recognition technology: %0a%3c https://ai.googleblog.com/2019/08/on-device-real-time-hand-tracking-with.html%0a%3c %0a%3c Sources: %0a%3c https://towardsdatascience.com/sign-language-recognition-using-deep-learning-6549268c60bd%0a%3c https://paperswithcode.com/task/sign-language-recognition%0a%3c %0a%3c %0a%3c [[#contentends]]%0a%3c ----%0a%3c (:Category:)%0a\ No newline at end of file%0a
host:1620443740=74.109.251.161
