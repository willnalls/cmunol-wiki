version=pmwiki-2.2.130 ordered=1 urlencoded=1
agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36
author=TA_mason
charset=UTF-8
csum=
ctime=1620521720
host=72.95.222.76
name=GradebookExpansions.BagOfWordsModelInNaturalLanguageProcessing-RussellE
rev=3
targets=Articles.BagOfWordsModelInNaturalLanguageProcessing,Profiles.Sebastiang
text=:Category: Varia%0a:Expansion: Byte Pair Encoding%0a:Author: Russell_E%0a:Original: [[Articles.BagOfWordsModelInNaturalLanguageProcessing|BagOfWordsModelInNaturalLanguageProcessing]] %0a:OriginalAuthor: [[Profiles.sebastiang|sebastiang]]%0a:Section: E%0a:Completed: 08.05.2021 - 17:55%0a:Status: complete%0a%0a(:foxform Site.FoxForms#gradeExpansion:)%0a(:foxform Site.FoxForms#gradingcomment:)%0a%0a%0a%25red%25 '''Grading History'''%0a%0a[[#history]]%0a#foxbegin 210512-120802-904280#%0a* [-12.05.2021 - 05:08-] || TA_mason marked as complete%0a#foxend 210512-120802-904280#%0a[[#historyend]]%0a%0a%25red%25 '''Comments to student'''%0a%0a[[#comments]]%0a%0a[[#commentsend]]%0a%0a----%0a----%0a!!%0a%0a%0a----%0a[[#content]]%0a%0aNick mentions a generalization of the bigram model to an n-gram model. One way to further generalize this is a concept called byte pair encoding. This model is built from the bottom up. A fun application of BPE is shown in this video:%0ahttps://www.youtube.com/watch?v=_ry6S-Dc2X8%0a%0aIn the video, BPE is explained in terms of characters. The model starts with individual characters as interesting strings. Then, it finds the most common combination of two interesting strings and adds it as a new interesting string. This can be repeated however many times as desired. %0a%0aBPE was originally created as a form of data compression. More interesting strings makes the original message shorter when encoded, but storing all the interesting strings eventually outweighs it if there are too many. However, it also has very useful applications in natural language processing when it's mixed with machine learning. When based on characters, BPE not only can recognize words, but can also make complex morphological analysis. Changing the number of interesting strings qualitatively changes analysis. If BPE is applied with words as the starting units, could it recognize high-level syntactic patterns? It's an interesting possibility.%0a%0aIt's also explained here:%0ahttps://en.wikipedia.org/wiki/Byte_pair_encoding%0a%0a[[#contentends]]%0a----%0a(:Category:)%0a#foxbegin 210512-000933-97660#%0a!!Backup interface%0a(:foxform Site.FoxForms#gradeExpansion:)%0a#foxend 210512-000933-97660#%0a(:GradedBy: TA_mason:)
time=1620821282
author:1620821282=TA_mason
diff:1620821282:1620778172:=8,9c8,9%0a%3c :Status: complete%0a%3c %0a---%0a> :Status: ungraded%0a> %0a17,19c17%0a%3c #foxbegin 210512-120802-904280#%0a%3c * [-12.05.2021 - 05:08-] || TA_mason marked as complete%0a%3c #foxend 210512-120802-904280#%0a---%0a> %0a52,53c50%0a%3c #foxend 210512-000933-97660#%0a%3c (:GradedBy: TA_mason:)%0a\ No newline at end of file%0a---%0a> #foxend 210512-000933-97660#%0a\ No newline at end of file%0a
host:1620821282=72.95.222.76
author:1620778172=
diff:1620778172:1620521720:=46,50c46%0a%3c (:Category:)%0a%3c #foxbegin 210512-000933-97660#%0a%3c !!Backup interface%0a%3c (:foxform Site.FoxForms#gradeExpansion:)%0a%3c #foxend 210512-000933-97660#%0a\ No newline at end of file%0a---%0a> (:Category:)%0a\ No newline at end of file%0a
host:1620778172=74.109.239.200
author:1620521720=Russell_E
diff:1620521720:1620521720:=1,46d0%0a%3c :Category: Varia%0a%3c :Expansion: Byte Pair Encoding%0a%3c :Author: Russell_E%0a%3c :Original: [[Articles.BagOfWordsModelInNaturalLanguageProcessing|BagOfWordsModelInNaturalLanguageProcessing]] %0a%3c :OriginalAuthor: [[Profiles.sebastiang|sebastiang]]%0a%3c :Section: E%0a%3c :Completed: 08.05.2021 - 17:55%0a%3c :Status: ungraded%0a%3c %0a%3c (:foxform Site.FoxForms#gradeExpansion:)%0a%3c (:foxform Site.FoxForms#gradingcomment:)%0a%3c %0a%3c %0a%3c %25red%25 '''Grading History'''%0a%3c %0a%3c [[#history]]%0a%3c %0a%3c [[#historyend]]%0a%3c %0a%3c %25red%25 '''Comments to student'''%0a%3c %0a%3c [[#comments]]%0a%3c %0a%3c [[#commentsend]]%0a%3c %0a%3c ----%0a%3c ----%0a%3c !!%0a%3c %0a%3c %0a%3c ----%0a%3c [[#content]]%0a%3c %0a%3c Nick mentions a generalization of the bigram model to an n-gram model. One way to further generalize this is a concept called byte pair encoding. This model is built from the bottom up. A fun application of BPE is shown in this video:%0a%3c https://www.youtube.com/watch?v=_ry6S-Dc2X8%0a%3c %0a%3c In the video, BPE is explained in terms of characters. The model starts with individual characters as interesting strings. Then, it finds the most common combination of two interesting strings and adds it as a new interesting string. This can be repeated however many times as desired. %0a%3c %0a%3c BPE was originally created as a form of data compression. More interesting strings makes the original message shorter when encoded, but storing all the interesting strings eventually outweighs it if there are too many. However, it also has very useful applications in natural language processing when it's mixed with machine learning. When based on characters, BPE not only can recognize words, but can also make complex morphological analysis. Changing the number of interesting strings qualitatively changes analysis. If BPE is applied with words as the starting units, could it recognize high-level syntactic patterns? It's an interesting possibility.%0a%3c %0a%3c It's also explained here:%0a%3c https://en.wikipedia.org/wiki/Byte_pair_encoding%0a%3c %0a%3c [[#contentends]]%0a%3c ----%0a%3c (:Category:)%0a\ No newline at end of file%0a
host:1620521720=99.171.140.109
