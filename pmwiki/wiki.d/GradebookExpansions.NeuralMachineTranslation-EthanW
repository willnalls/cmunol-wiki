version=pmwiki-2.2.130 ordered=1 urlencoded=1
agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36
author=TA_mason
charset=UTF-8
csum=
ctime=1619702260
host=74.98.251.138
name=GradebookExpansions.NeuralMachineTranslation-EthanW
rev=3
targets=Articles.NeuralMachineTranslation,Profiles.Aryans
text=:Category: Varia%0a:Expansion: Tokenization%0a:Author: EthanW%0a:Original: [[Articles.NeuralMachineTranslation|NeuralMachineTranslation]] %0a:OriginalAuthor: [[Profiles.aryans|aryans]]%0a:Section: E%0a:Completed: 29.04.2021 - 06:17%0a:Status: complete%0a%0a(:foxform Site.FoxForms#gradeExpansion:)%0a(:foxform Site.FoxForms#gradingcomment:)%0a%0a%0a%25red%25 '''Grading History'''%0a%0a[[#history]]%0a#foxbegin 210505-220837-293360#%0a* [-05.05.2021 - 15:08-] || TA_mason marked as complete%0a#foxend 210505-220837-293360#%0a#foxbegin 210505-220833-377790#%0a* [-05.05.2021 - 15:08-] || TA_mason marked as complete%0a#foxend 210505-220833-377790#%0a[[#historyend]]%0a%0a%25red%25 '''Comments to student'''%0a%0a[[#comments]]%0a%0a[[#commentsend]]%0a%0a----%0a----%0a!!%0a%0a%0a----%0a[[#content]]%0a%0aIn the last paragraph, you mentioned that the structure of input data would impact the neural translator's performance. In fact, converting strings of sentences into word-like units as input to the model involves a process called tokenization. More formally, tokenization splits a phrase, sentence, paragraph, or an entire text document into smaller units, such as individual words or terms. For language like English, it is usually easy to identify units of a sentence by blank space. As for Mandarin Chinese, however, it is harder to determine where to make a split. %0a%0aIn many state-of-the-art computational linguistic models, tokenization is data-driven based on frequency, dependency, etc. There are also many well-documented implementations that enable typical users to perform tokenization very efficiently (see "source" for more details). %0a%0aSource: https://en.wikipedia.org/wiki/Lexical_analysis#Tokenization; %0ahttps://github.com/huggingface/tokenizers; %0ahttps://www.analyticsvidhya.com/blog/2019/07/how-get-started-nlp-6-unique-ways-perform-tokenization/%0a%0a[[#contentends]]%0a----%0a(:Category:)%0a(:GradedBy: TA_mason:)
time=1620252516
author:1620252516=TA_mason
diff:1620252516:1620252512:=17,19d16%0a%3c #foxbegin 210505-220837-293360#%0a%3c * [-05.05.2021 - 15:08-] || TA_mason marked as complete%0a%3c #foxend 210505-220837-293360#%0a
host:1620252516=74.98.251.138
author:1620252512=TA_mason
diff:1620252512:1619702260:=8,9c8,9%0a%3c :Status: complete%0a%3c %0a---%0a> :Status: ungraded%0a> %0a17,19c17%0a%3c #foxbegin 210505-220833-377790#%0a%3c * [-05.05.2021 - 15:08-] || TA_mason marked as complete%0a%3c #foxend 210505-220833-377790#%0a---%0a> %0a46,47c44%0a%3c (:Category:)%0a%3c (:GradedBy: TA_mason:)%0a\ No newline at end of file%0a---%0a> (:Category:)%0a\ No newline at end of file%0a
host:1620252512=74.98.251.138
author:1619702260=EthanW
diff:1619702260:1619702260:=1,44d0%0a%3c :Category: Varia%0a%3c :Expansion: Tokenization%0a%3c :Author: EthanW%0a%3c :Original: [[Articles.NeuralMachineTranslation|NeuralMachineTranslation]] %0a%3c :OriginalAuthor: [[Profiles.aryans|aryans]]%0a%3c :Section: E%0a%3c :Completed: 29.04.2021 - 06:17%0a%3c :Status: ungraded%0a%3c %0a%3c (:foxform Site.FoxForms#gradeExpansion:)%0a%3c (:foxform Site.FoxForms#gradingcomment:)%0a%3c %0a%3c %0a%3c %25red%25 '''Grading History'''%0a%3c %0a%3c [[#history]]%0a%3c %0a%3c [[#historyend]]%0a%3c %0a%3c %25red%25 '''Comments to student'''%0a%3c %0a%3c [[#comments]]%0a%3c %0a%3c [[#commentsend]]%0a%3c %0a%3c ----%0a%3c ----%0a%3c !!%0a%3c %0a%3c %0a%3c ----%0a%3c [[#content]]%0a%3c %0a%3c In the last paragraph, you mentioned that the structure of input data would impact the neural translator's performance. In fact, converting strings of sentences into word-like units as input to the model involves a process called tokenization. More formally, tokenization splits a phrase, sentence, paragraph, or an entire text document into smaller units, such as individual words or terms. For language like English, it is usually easy to identify units of a sentence by blank space. As for Mandarin Chinese, however, it is harder to determine where to make a split. %0a%3c %0a%3c In many state-of-the-art computational linguistic models, tokenization is data-driven based on frequency, dependency, etc. There are also many well-documented implementations that enable typical users to perform tokenization very efficiently (see "source" for more details). %0a%3c %0a%3c Source: https://en.wikipedia.org/wiki/Lexical_analysis#Tokenization; %0a%3c https://github.com/huggingface/tokenizers; %0a%3c https://www.analyticsvidhya.com/blog/2019/07/how-get-started-nlp-6-unique-ways-perform-tokenization/%0a%3c %0a%3c [[#contentends]]%0a%3c ----%0a%3c (:Category:)%0a\ No newline at end of file%0a
host:1619702260=58.247.22.142
