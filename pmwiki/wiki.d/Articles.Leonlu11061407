version=pmwiki-2.2.130 ordered=1 urlencoded=1
agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36
author=ziqiliu
charset=UTF-8
csum=
ctime=1667772500
host=96.44.9.107
name=Articles.Leonlu11061407
rev=8
targets=GradebookArticles.Leonlu11061407,Articles.Leonlu11061407,Category.Varia
text=(:if [ authgroup @tas || authgroup @admins || equal {$Author} '' ]:)%0a%0a(:toggle id=gi show="show grading interface" hide="hide grading interface":)%0a%0a>>id=gi%3c%3c%0a%0a[[GradebookArticles.{$Name}|See article in gradebook]] \\%0a[[{$FullName}?action=diff|See all changes to article]] \\%0a[[{$FullName}?action=edit|Edit this page]] \\%0aStatus: {GradebookArticles.{$Name}$:Status} %0a%0a(:foxform Site.FoxForms#gradeitem:)%0a%0a(:foxform Site.FoxForms#gradingcomment:)%0a%0a>>%3c%3c%0a%0a----%0a%0a(:ifend:)%0a%0a[[!{$:Category}]]%0a%0a!Computational Linguistics and N-Gram Model%0a%0a:Author: leonlu%0a%0a'''Summary:''' \\%0a%0a[[#summary]]%0a%0aAn exploration of n-gram models%0a%0a[[#summaryends]]%0a%0a----%0a[[#content]]%0a%0aAs early as the late 1940s, people have attempted to use computers to process natural languages, namely to translate them (i.e., human language that developed naturally, rather than invented languages or programming languages). Computers are machines that are good at numbers but not abstract words that as we understand them, so one problem researchers faced was representing human language in a way that computers can understand. %0a%0aOne way to model language is by using statistics. To introduce the topic, let's imagine we are designing a system that predicts the next word in a sentence. In fact, this is probably something your phone does as you type. One way to do it is to just return the most likely word in the language. Though it's probably a bad way, let's consider what we get. %0a%0aLet's say, in language X, there are only four words: "the", "fresh", "snow", and "peas" (this is called the vocabulary of the language), and let's say in an arbitrary corpus of language X, the probability of seeing these words are as follows:%0a%0a| word  | probability |  %0a| ----- | ----------- |  %0a| the   | 56%25         |  %0a| fresh | 26%25         |  %0a| snow  | 10%25         |  %0a| peas  | 6%25          |  %0a%0a(sorry it doesn't seem to render tables properly...)%0a%0aAccording to our approach, our model would just predict "the" every time we type something, even though it's unlikely that we will use "the" ten times in a row. This approach sounds stupid, but it is actually accurate 56%25 of the time—much better than 25%25 if we randomly return a word. Computational linguistics call such model a 1-gram (or unigram) model, since it only cares about one word at the time. %0a%0aWhen we type, our next word often depends on the previous word, so why don't we increase the window the computer looks at the previous word(s) when predicting the next word. Expanding the search window by 1, we get a 2-gram (or bigram) model, and the probably of the next word will be calculated based on what the previous word is. For example, if we have "the fresh ____" and ask the computer to fill in the blank, it will look for the probability in this table:%0a%0a| word + blank  | probability |%0a| ------------- | ----------- |%0a| fresh _the_   | 10%25         |%0a| fresh _fresh_ | 3%25         |%0a| fresh _snow_  | 61%25         |%0a| fresh _peas_  | 26%25          |%0a%0aAnd viola—the computer fills in the blank with "snow"! With a 1-gram model, the computer would have said "the fresh _the_". Clearly, increasing our window gives us a more precise answer. We can go further with higher value of n. For example, a 4-gram model predicts the next words based on the previous 3 words, and a 7-gram model predicts the next words based on the previous 6 words. %0a%0aHowever, a problem arises when we keep going like this. With a nature language, the we will have a much higher vocabulary, and each time we increase the window size, we multiply the number of known sequences by the vocabulary size. So for the language with v words, the table size of a n-gram model would be O(v^n)—not very efficient.%0a%0aAre there more efficient ways to represent language? Yes, but that would be the topic of another article. (Hint: vectors)%0a%0a1. https://www.kaggle.com/code/pavansanagapati/knowledge-graph-nlp-tutorial-bert-spacy-nltk/notebook%0a2. Stanford CS224N: Natural Language Processing with Deep Learning, Winter 2019, Lecture 1%0a3. https://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes01-wordvecs1.pdf%0a4. Stanford CS224N: Natural Language Processing with Deep Learning, Winter 2019, Lecture 2%0a%0a[[#contentends]]%0a----%0a----%0a%0a!!Expansions:%0a[[#expansions]]%0a#foxbegin 221112-222442-760080#%0a(:div1 class=expansionhead:)%0a!!!text similarity %0a-> - ejwang2%0a-> [-12.11.2022 - 14:24-]%0a>>rfloat%3c%3c   %0a(:if1 authgroup @tas:)%0a{[foxdelrange button 221112-222442-760080 {$FullName} ]}%0a(:if1end:)%0a>>%3c%3c%0a(:div1end:) %0a>>messageitem%3c%3c %0aIndeed, many natural language processing libraries employ word vectors to encode semantic meaning. The sweeping importance of detecting similarity in text is obvious, but unfortunately many state of the art models are not always able to do the best job. For example, you can experiment with Python's spacy package text similarity here:%0a%0ahttps://spacy.io/usage/linguistic-features#vectors-similarity%0a%0aIt doesn't take a lot of experimentation to come up with some unsatisfactory similarity measurements (for example, two sentences which are very clearly semantically similar to humans and yet are assigned an ambiguous similarity value by spacy). %0a>>%3c%3c%0a#foxend 221112-222442-760080#%0a#foxbegin 221210-010847-563120#%0a(:div1 class=expansionhead:)%0a!!!Recent progress since N-Gram model %0a-> - ziqiliu%0a-> [-09.12.2022 - 17:08-]%0a>>rfloat%3c%3c   %0a(:if1 authgroup @tas:)%0a{[foxdelrange button 221210-010847-563120 {$FullName} ]}%0a(:if1end:)%0a>>%3c%3c%0a(:div1end:) %0a>>messageitem%3c%3c %0aThe next landmark language-model is the Transformer model invented in 2017. The original paper is titled "Attention is All You Need," which somewhat summarizes what this idea is all about: Attention. Attention is a mechanism that encodes the semantic meaning of a sentence as statistical weights to different components within the sentence (analogous to how a reader would pay more attention to certain words or phrases than others in order to understand a sentence). The transformer model lays the foundations of later well-known advancements (e.g., GPT‘s). %0a%0ahttps://www.analyticsvidhya.com/blog/2019/06/understanding-transformers-nlp-state-of-the-art-models/%0a>>%3c%3c%0a#foxend 221210-010847-563120#%0a[[#expansionsend]]%0a%0a----%0a%25red%25 '''Add an expansion:'''%0a%0a(:if [ exists GradebookExpansions.{$Name}-{$Author} || equal {$Author} {$:Author} ] :)%0a%0a(:elseif [ equal {Profiles.{$Author}$:andrewid} '' || equal {Profiles.{$Author}$:section} '' ] :)%0a%0aYou must enter your andrew ID and section before submitting an expansion. %0a%0a(:else:)%0a%0a(:foxform Site.FoxForms#newexpansion:)%0a%0a(:ifend:)%0a%0a----%0a----%0a%0a!!Comments%0a%0a%0a%0a%0a----%0a(:if equal {$Author} {$:Author}:)%0a!!Change article category%0a(:foxform Site.FoxForms#changearticlecategory:)%0a(:ifend:)%0a%0a%0a(:Section: F:)%0a(:Category: Varia:)%0a(:Title: Computational Linguistics and N-Gram Model:)
time=1670634527
author:1670634527=ziqiliu
diff:1670634527:1668291882:=101,117d100%0a%3c #foxbegin 221210-010847-563120#%0a%3c (:div1 class=expansionhead:)%0a%3c !!!Recent progress since N-Gram model %0a%3c -> - ziqiliu%0a%3c -> [-09.12.2022 - 17:08-]%0a%3c >>rfloat%3c%3c   %0a%3c (:if1 authgroup @tas:)%0a%3c {[foxdelrange button 221210-010847-563120 {$FullName} ]}%0a%3c (:if1end:)%0a%3c >>%3c%3c%0a%3c (:div1end:) %0a%3c >>messageitem%3c%3c %0a%3c The next landmark language-model is the Transformer model invented in 2017. The original paper is titled "Attention is All You Need," which somewhat summarizes what this idea is all about: Attention. Attention is a mechanism that encodes the semantic meaning of a sentence as statistical weights to different components within the sentence (analogous to how a reader would pay more attention to certain words or phrases than others in order to understand a sentence). The transformer model lays the foundations of later well-known advancements (e.g., GPT‘s). %0a%3c %0a%3c https://www.analyticsvidhya.com/blog/2019/06/understanding-transformers-nlp-state-of-the-art-models/%0a%3c >>%3c%3c%0a%3c #foxend 221210-010847-563120#%0a
host:1670634527=96.44.9.107
author:1668291882=ejwang2
diff:1668291882:1667843530:=82,100c82%0a%3c #foxbegin 221112-222442-760080#%0a%3c (:div1 class=expansionhead:)%0a%3c !!!text similarity %0a%3c -> - ejwang2%0a%3c -> [-12.11.2022 - 14:24-]%0a%3c >>rfloat%3c%3c   %0a%3c (:if1 authgroup @tas:)%0a%3c {[foxdelrange button 221112-222442-760080 {$FullName} ]}%0a%3c (:if1end:)%0a%3c >>%3c%3c%0a%3c (:div1end:) %0a%3c >>messageitem%3c%3c %0a%3c Indeed, many natural language processing libraries employ word vectors to encode semantic meaning. The sweeping importance of detecting similarity in text is obvious, but unfortunately many state of the art models are not always able to do the best job. For example, you can experiment with Python's spacy package text similarity here:%0a%3c %0a%3c https://spacy.io/usage/linguistic-features#vectors-similarity%0a%3c %0a%3c It doesn't take a lot of experimentation to come up with some unsatisfactory similarity measurements (for example, two sentences which are very clearly semantically similar to humans and yet are assigned an ambiguous similarity value by spacy). %0a%3c >>%3c%3c%0a%3c #foxend 221112-222442-760080#%0a---%0a> %0a
host:1668291882=128.237.82.1
author:1667843530=leonlu
diff:1667843530:1667843523:=116c116%0a%3c (:Category: Varia:)%0a---%0a> (:Category: TypologyOfPhoneticContrasts:)%0a
host:1667843530=128.237.82.5
author:1667843523=leonlu
diff:1667843523:1667772658:=116c116%0a%3c (:Category: TypologyOfPhoneticContrasts:)%0a---%0a> (:Category: MorphosyntacticDiversityOfEnglish:)%0a
host:1667843523=128.237.82.5
author:1667772658=leonlu
diff:1667772658:1667772580:=45,52c45,71%0a%3c | word  | probability |  %0a%3c | ----- | ----------- |  %0a%3c | the   | 56%25         |  %0a%3c | fresh | 26%25         |  %0a%3c | snow  | 10%25         |  %0a%3c | peas  | 6%25          |  %0a%3c %0a%3c (sorry it doesn't seem to render tables properly...)%0a---%0a> %3ctable>%0a>     %3ctr>%0a>         %3ctd>word%3c/td>%0a>         %3ctd>probability%3c/td>%0a>         %3ctd>%3c/td>%0a>     %3c/tr>%0a>     %3ctr>%0a>         %3ctd>the%3c/td>%0a>         %3ctd>56%25%3c/td>%0a>         %3ctd>%3c/td>%0a>     %3c/tr>%0a>     %3ctr>%0a>         %3ctd>fresh%3c/td>%0a>         %3ctd>26%25%3c/td>%0a>         %3ctd>%3c/td>%0a>     %3c/tr>%0a>     %3ctr>%0a>         %3ctd>snow%3c/td>%0a>         %3ctd>10%25%3c/td>%0a>         %3ctd>%3c/td>%0a>     %3c/tr>%0a>     %3ctr>%0a>         %3ctd>peas%3c/td>%0a>         %3ctd>6%25%3c/td>%0a>         %3ctd>%3c/td>%0a>     %3c/tr>%0a> %3c/table>%0a
host:1667772658=128.237.82.5
author:1667772580=leonlu
diff:1667772580:1667772537:=45,71c45,50%0a%3c %3ctable>%0a%3c     %3ctr>%0a%3c         %3ctd>word%3c/td>%0a%3c         %3ctd>probability%3c/td>%0a%3c         %3ctd>%3c/td>%0a%3c     %3c/tr>%0a%3c     %3ctr>%0a%3c         %3ctd>the%3c/td>%0a%3c         %3ctd>56%25%3c/td>%0a%3c         %3ctd>%3c/td>%0a%3c     %3c/tr>%0a%3c     %3ctr>%0a%3c         %3ctd>fresh%3c/td>%0a%3c         %3ctd>26%25%3c/td>%0a%3c         %3ctd>%3c/td>%0a%3c     %3c/tr>%0a%3c     %3ctr>%0a%3c         %3ctd>snow%3c/td>%0a%3c         %3ctd>10%25%3c/td>%0a%3c         %3ctd>%3c/td>%0a%3c     %3c/tr>%0a%3c     %3ctr>%0a%3c         %3ctd>peas%3c/td>%0a%3c         %3ctd>6%25%3c/td>%0a%3c         %3ctd>%3c/td>%0a%3c     %3c/tr>%0a%3c %3c/table>%0a---%0a> | word  | probability |  %0a> | ----- | ----------- |  %0a> | the   | 56%25         |  %0a> | fresh | 26%25         |  %0a> | snow  | 10%25         |  %0a> | peas  | 6%25          |  %0a
host:1667772580=128.237.82.5
author:1667772537=leonlu
diff:1667772537:1667772500:=39,71c39,71%0a%3c As early as the late 1940s, people have attempted to use computers to process natural languages, namely to translate them (i.e., human language that developed naturally, rather than invented languages or programming languages). Computers are machines that are good at numbers but not abstract words that as we understand them, so one problem researchers faced was representing human language in a way that computers can understand. %0a%3c %0a%3c One way to model language is by using statistics. To introduce the topic, let's imagine we are designing a system that predicts the next word in a sentence. In fact, this is probably something your phone does as you type. One way to do it is to just return the most likely word in the language. Though it's probably a bad way, let's consider what we get. %0a%3c %0a%3c Let's say, in language X, there are only four words: "the", "fresh", "snow", and "peas" (this is called the vocabulary of the language), and let's say in an arbitrary corpus of language X, the probability of seeing these words are as follows:%0a%3c %0a%3c | word  | probability |  %0a%3c | ----- | ----------- |  %0a%3c | the   | 56%25         |  %0a%3c | fresh | 26%25         |  %0a%3c | snow  | 10%25         |  %0a%3c | peas  | 6%25          |  %0a%3c %0a%3c According to our approach, our model would just predict "the" every time we type something, even though it's unlikely that we will use "the" ten times in a row. This approach sounds stupid, but it is actually accurate 56%25 of the time—much better than 25%25 if we randomly return a word. Computational linguistics call such model a 1-gram (or unigram) model, since it only cares about one word at the time. %0a%3c %0a%3c When we type, our next word often depends on the previous word, so why don't we increase the window the computer looks at the previous word(s) when predicting the next word. Expanding the search window by 1, we get a 2-gram (or bigram) model, and the probably of the next word will be calculated based on what the previous word is. For example, if we have "the fresh ____" and ask the computer to fill in the blank, it will look for the probability in this table:%0a%3c %0a%3c | word + blank  | probability |%0a%3c | ------------- | ----------- |%0a%3c | fresh _the_   | 10%25         |%0a%3c | fresh _fresh_ | 3%25         |%0a%3c | fresh _snow_  | 61%25         |%0a%3c | fresh _peas_  | 26%25          |%0a%3c %0a%3c And viola—the computer fills in the blank with "snow"! With a 1-gram model, the computer would have said "the fresh _the_". Clearly, increasing our window gives us a more precise answer. We can go further with higher value of n. For example, a 4-gram model predicts the next words based on the previous 3 words, and a 7-gram model predicts the next words based on the previous 6 words. %0a%3c %0a%3c However, a problem arises when we keep going like this. With a nature language, the we will have a much higher vocabulary, and each time we increase the window size, we multiply the number of known sequences by the vocabulary size. So for the language with v words, the table size of a n-gram model would be O(v^n)—not very efficient.%0a%3c %0a%3c Are there more efficient ways to represent language? Yes, but that would be the topic of another article. (Hint: vectors)%0a%3c %0a%3c 1. https://www.kaggle.com/code/pavansanagapati/knowledge-graph-nlp-tutorial-bert-spacy-nltk/notebook%0a%3c 2. Stanford CS224N: Natural Language Processing with Deep Learning, Winter 2019, Lecture 1%0a%3c 3. https://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes01-wordvecs1.pdf%0a---%0a> As early as the late 1940s, people have attempted to use computers to process natural languages, namely to translate them (i.e., human language that developed naturally, rather than invented languages or programming languages). Computers are machines that are good at numbers but not abstract words that as we understand them, so one problem researchers faced was representing human language in a way that computers can understand. %0a> %0a> One way to model language is by using statistics. To introduce the topic, let's imagine we are designing a system that predicts the next word in a sentence. In fact, this is probably something your phone does as you type. One way to do it is to just return the most likely word in the language. Though it's probably a bad way, let's consider what we get. %0a> %0a> Let's say, in language X, there are only four words: "the", "fresh", "snow", and "peas" (this is called the vocabulary of the language), and let's say in an arbitrary corpus of language X, the probability of seeing these words are as follows:%0a> %0a> | word  | probability |%0a> | ----- | ----------- |%0a> | the   | 56%25         |%0a> | fresh | 26%25         |%0a> | snow  | 10%25         |%0a> | peas  | 6%25          |%0a> %0a> According to our approach, our model would just predict "the" every time we type something, even though it's unlikely that we will use "the" ten times in a row. This approach sounds stupid, but it is actually accurate 56%25 of the time—much better than 25%25 if we randomly return a word. Computational linguistics call such model a 1-gram (or unigram) model, since it only cares about one word at the time. %0a> %0a> When we type, our next word often depends on the previous word, so why don't we increase the window the computer looks at the previous word(s) when predicting the next word. Expanding the search window by 1, we get a 2-gram (or bigram) model, and the probably of the next word will be calculated based on what the previous word is. For example, if we have "the fresh ____" and ask the computer to fill in the blank, it will look for the probability in this table:%0a> %0a> | word + blank  | probability |%0a> | ------------- | ----------- |%0a> | fresh _the_   | 10%25         |%0a> | fresh _fresh_ | 3%25         |%0a> | fresh _snow_  | 61%25         |%0a> | fresh _peas_  | 26%25          |%0a> %0a> And viola—the computer fills in the blank with "snow"! With a 1-gram model, the computer would have said "the fresh _the_". Clearly, increasing our window gives us a more precise answer. We can go further with higher value of n. For example, a 4-gram model predicts the next words based on the previous 3 words, and a 7-gram model predicts the next words based on the previous 6 words. %0a> %0a> However, a problem arises when we keep going like this. With a nature language, the we will have a much higher vocabulary, and each time we increase the window size, we multiply the number of known sequences by the vocabulary size. So for the language with v words, the table size of a n-gram model would be O(v^n)—not very efficient.%0a> %0a> Are there more efficient ways to represent language? Yes, but that would be the topic of another article. (Hint: vectors)%0a> %0a> 1. https://www.kaggle.com/code/pavansanagapati/knowledge-graph-nlp-tutorial-bert-spacy-nltk/notebook%0a> 2. Stanford CS224N: Natural Language Processing with Deep Learning, Winter 2019, Lecture 1%0a> 3. https://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes01-wordvecs1.pdf%0a
host:1667772537=128.237.82.5
author:1667772500=leonlu
diff:1667772500:1667772500:=1,115d0%0a%3c (:if [ authgroup @tas || authgroup @admins || equal {$Author} '' ]:)%0a%3c %0a%3c (:toggle id=gi show="show grading interface" hide="hide grading interface":)%0a%3c %0a%3c >>id=gi%3c%3c%0a%3c %0a%3c [[GradebookArticles.{$Name}|See article in gradebook]] \\%0a%3c [[{$FullName}?action=diff|See all changes to article]] \\%0a%3c [[{$FullName}?action=edit|Edit this page]] \\%0a%3c Status: {GradebookArticles.{$Name}$:Status} %0a%3c %0a%3c (:foxform Site.FoxForms#gradeitem:)%0a%3c %0a%3c (:foxform Site.FoxForms#gradingcomment:)%0a%3c %0a%3c >>%3c%3c%0a%3c %0a%3c ----%0a%3c %0a%3c (:ifend:)%0a%3c %0a%3c [[!{$:Category}]]%0a%3c %0a%3c !Computational Linguistics and N-Gram Model%0a%3c %0a%3c :Author: leonlu%0a%3c %0a%3c '''Summary:''' \\%0a%3c %0a%3c [[#summary]]%0a%3c %0a%3c An exploration of n-gram models%0a%3c %0a%3c [[#summaryends]]%0a%3c %0a%3c ----%0a%3c [[#content]]%0a%3c %0a%3c As early as the late 1940s, people have attempted to use computers to process natural languages, namely to translate them (i.e., human language that developed naturally, rather than invented languages or programming languages). Computers are machines that are good at numbers but not abstract words that as we understand them, so one problem researchers faced was representing human language in a way that computers can understand. %0a%3c %0a%3c One way to model language is by using statistics. To introduce the topic, let's imagine we are designing a system that predicts the next word in a sentence. In fact, this is probably something your phone does as you type. One way to do it is to just return the most likely word in the language. Though it's probably a bad way, let's consider what we get. %0a%3c %0a%3c Let's say, in language X, there are only four words: "the", "fresh", "snow", and "peas" (this is called the vocabulary of the language), and let's say in an arbitrary corpus of language X, the probability of seeing these words are as follows:%0a%3c %0a%3c | word  | probability |%0a%3c | ----- | ----------- |%0a%3c | the   | 56%25         |%0a%3c | fresh | 26%25         |%0a%3c | snow  | 10%25         |%0a%3c | peas  | 6%25          |%0a%3c %0a%3c According to our approach, our model would just predict "the" every time we type something, even though it's unlikely that we will use "the" ten times in a row. This approach sounds stupid, but it is actually accurate 56%25 of the time—much better than 25%25 if we randomly return a word. Computational linguistics call such model a 1-gram (or unigram) model, since it only cares about one word at the time. %0a%3c %0a%3c When we type, our next word often depends on the previous word, so why don't we increase the window the computer looks at the previous word(s) when predicting the next word. Expanding the search window by 1, we get a 2-gram (or bigram) model, and the probably of the next word will be calculated based on what the previous word is. For example, if we have "the fresh ____" and ask the computer to fill in the blank, it will look for the probability in this table:%0a%3c %0a%3c | word + blank  | probability |%0a%3c | ------------- | ----------- |%0a%3c | fresh _the_   | 10%25         |%0a%3c | fresh _fresh_ | 3%25         |%0a%3c | fresh _snow_  | 61%25         |%0a%3c | fresh _peas_  | 26%25          |%0a%3c %0a%3c And viola—the computer fills in the blank with "snow"! With a 1-gram model, the computer would have said "the fresh _the_". Clearly, increasing our window gives us a more precise answer. We can go further with higher value of n. For example, a 4-gram model predicts the next words based on the previous 3 words, and a 7-gram model predicts the next words based on the previous 6 words. %0a%3c %0a%3c However, a problem arises when we keep going like this. With a nature language, the we will have a much higher vocabulary, and each time we increase the window size, we multiply the number of known sequences by the vocabulary size. So for the language with v words, the table size of a n-gram model would be O(v^n)—not very efficient.%0a%3c %0a%3c Are there more efficient ways to represent language? Yes, but that would be the topic of another article. (Hint: vectors)%0a%3c %0a%3c 1. https://www.kaggle.com/code/pavansanagapati/knowledge-graph-nlp-tutorial-bert-spacy-nltk/notebook%0a%3c 2. Stanford CS224N: Natural Language Processing with Deep Learning, Winter 2019, Lecture 1%0a%3c 3. https://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes01-wordvecs1.pdf%0a%3c 4. Stanford CS224N: Natural Language Processing with Deep Learning, Winter 2019, Lecture 2%0a%3c %0a%3c [[#contentends]]%0a%3c ----%0a%3c ----%0a%3c %0a%3c !!Expansions:%0a%3c [[#expansions]]%0a%3c %0a%3c [[#expansionsend]]%0a%3c %0a%3c ----%0a%3c %25red%25 '''Add an expansion:'''%0a%3c %0a%3c (:if [ exists GradebookExpansions.{$Name}-{$Author} || equal {$Author} {$:Author} ] :)%0a%3c %0a%3c (:elseif [ equal {Profiles.{$Author}$:andrewid} '' || equal {Profiles.{$Author}$:section} '' ] :)%0a%3c %0a%3c You must enter your andrew ID and section before submitting an expansion. %0a%3c %0a%3c (:else:)%0a%3c %0a%3c (:foxform Site.FoxForms#newexpansion:)%0a%3c %0a%3c (:ifend:)%0a%3c %0a%3c ----%0a%3c ----%0a%3c %0a%3c !!Comments%0a%3c %0a%3c %0a%3c %0a%3c %0a%3c ----%0a%3c (:if equal {$Author} {$:Author}:)%0a%3c !!Change article category%0a%3c (:foxform Site.FoxForms#changearticlecategory:)%0a%3c (:ifend:)%0a%3c %0a%3c %0a%3c (:Section: F:)%0a%3c (:Category: MorphosyntacticDiversityOfEnglish:)%0a%3c (:Title: Computational Linguistics and N-Gram Model:)%0a\ No newline at end of file%0a
host:1667772500=128.237.82.5
