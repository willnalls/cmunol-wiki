version=pmwiki-2.2.130 ordered=1 urlencoded=1
agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36
author=TA_mason
charset=UTF-8
csum=
ctime=1620186847
host=74.98.251.138
name=GradebookExpansions.BagOfWordsModelInNaturalLanguageProcessing-caseywalker
rev=2
targets=Articles.BagOfWordsModelInNaturalLanguageProcessing,Profiles.Sebastiang
text=:Category: Varia%0a:Expansion: Sentiment Analysis%0a:Author: caseywalker%0a:Original: [[Articles.BagOfWordsModelInNaturalLanguageProcessing|BagOfWordsModelInNaturalLanguageProcessing]] %0a:OriginalAuthor: [[Profiles.sebastiang|sebastiang]]%0a:Section: F%0a:Completed: 04.05.2021 - 20:54%0a:Status: complete%0a%0a(:foxform Site.FoxForms#gradeExpansion:)%0a(:foxform Site.FoxForms#gradingcomment:)%0a%0a%0a%25red%25 '''Grading History'''%0a%0a[[#history]]%0a#foxbegin 210505-221017-743090#%0a* [-05.05.2021 - 15:10-] || TA_mason marked as complete%0a#foxend 210505-221017-743090#%0a[[#historyend]]%0a%0a%25red%25 '''Comments to student'''%0a%0a[[#comments]]%0a%0a[[#commentsend]]%0a%0a----%0a----%0a!!%0a%0a%0a----%0a[[#content]]%0a%0aAnother application for something like this is sentiment analysis, where text is categorized by sentiment, or put into classes based on some training data, etc. It's actually my group's Numerical Methods topic for our semester project. We used what is basically a bag of words and implemented a Naïve Bayes model to try and categorize movie reviews as positive or negative based on their word frequencies. We came up with ~80%25 for finding positive reviews and ~90%25 for finding negative reviews using this.%0a%0aAn interesting (and presumably very challenging/computationally expensive) method that compounds on the bag of words model is lemmatizing (and/or stemming) words, so that words with the same meaning such as 'likes' and 'liked' would be considered the same word. I think I recall reading that it usually has minimal impact on predictions, however, so it's not super useful.%0a%0aOne last thing: you mentioned preposition words and such -- those are (and many more) are called 'stop' words and are generally removed from bag of words models because they add little to the meaning of sentences.%0a%0asources:%0a-https://sebastianraschka.com/Articles/2014_naive_bayes_1.html#n-grams%0a%0a[[#contentends]]%0a----%0a(:Category:)%0a(:GradedBy: TA_mason:)
time=1620252617
author:1620252617=TA_mason
diff:1620252617:1620186847:=8,9c8,9%0a%3c :Status: complete%0a%3c %0a---%0a> :Status: ungraded%0a> %0a17,19c17%0a%3c #foxbegin 210505-221017-743090#%0a%3c * [-05.05.2021 - 15:10-] || TA_mason marked as complete%0a%3c #foxend 210505-221017-743090#%0a---%0a> %0a47,48c45%0a%3c (:Category:)%0a%3c (:GradedBy: TA_mason:)%0a\ No newline at end of file%0a---%0a> (:Category:)%0a\ No newline at end of file%0a
host:1620252617=74.98.251.138
author:1620186847=caseywalker
diff:1620186847:1620186847:=1,45d0%0a%3c :Category: Varia%0a%3c :Expansion: Sentiment Analysis%0a%3c :Author: caseywalker%0a%3c :Original: [[Articles.BagOfWordsModelInNaturalLanguageProcessing|BagOfWordsModelInNaturalLanguageProcessing]] %0a%3c :OriginalAuthor: [[Profiles.sebastiang|sebastiang]]%0a%3c :Section: F%0a%3c :Completed: 04.05.2021 - 20:54%0a%3c :Status: ungraded%0a%3c %0a%3c (:foxform Site.FoxForms#gradeExpansion:)%0a%3c (:foxform Site.FoxForms#gradingcomment:)%0a%3c %0a%3c %0a%3c %25red%25 '''Grading History'''%0a%3c %0a%3c [[#history]]%0a%3c %0a%3c [[#historyend]]%0a%3c %0a%3c %25red%25 '''Comments to student'''%0a%3c %0a%3c [[#comments]]%0a%3c %0a%3c [[#commentsend]]%0a%3c %0a%3c ----%0a%3c ----%0a%3c !!%0a%3c %0a%3c %0a%3c ----%0a%3c [[#content]]%0a%3c %0a%3c Another application for something like this is sentiment analysis, where text is categorized by sentiment, or put into classes based on some training data, etc. It's actually my group's Numerical Methods topic for our semester project. We used what is basically a bag of words and implemented a Naïve Bayes model to try and categorize movie reviews as positive or negative based on their word frequencies. We came up with ~80%25 for finding positive reviews and ~90%25 for finding negative reviews using this.%0a%3c %0a%3c An interesting (and presumably very challenging/computationally expensive) method that compounds on the bag of words model is lemmatizing (and/or stemming) words, so that words with the same meaning such as 'likes' and 'liked' would be considered the same word. I think I recall reading that it usually has minimal impact on predictions, however, so it's not super useful.%0a%3c %0a%3c One last thing: you mentioned preposition words and such -- those are (and many more) are called 'stop' words and are generally removed from bag of words models because they add little to the meaning of sentences.%0a%3c %0a%3c sources:%0a%3c -https://sebastianraschka.com/Articles/2014_naive_bayes_1.html#n-grams%0a%3c %0a%3c [[#contentends]]%0a%3c ----%0a%3c (:Category:)%0a\ No newline at end of file%0a
host:1620186847=74.109.239.4
