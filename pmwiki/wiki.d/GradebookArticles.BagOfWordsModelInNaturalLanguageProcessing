version=pmwiki-2.2.130 ordered=1 urlencoded=1
agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36
author=TA_lili
charset=UTF-8
csum=
ctime=1619914459
host=74.111.97.246
name=GradebookArticles.BagOfWordsModelInNaturalLanguageProcessing
rev=3
targets=
text=:Category: Varia%0a:Essential: {Category.Varia$:essential}%0a:Title: Bag of Words Model in Natural Language Processing%0a:Author: sebastiang%0a:Section: F%0a:Completed: 01.05.2021 - 17:14%0a:Status: complete%0a%0a(:foxform Site.FoxForms#gradeitem:)%0a(:foxform Site.FoxForms#gradingcomment:)%0a%0a----%0a%0a%0a%25blue%25 '''Grading History'''%0a%0a[[#history]]%0a#foxbegin 210506-152451-304020#%0a* [-06.05.2021 - 08:24-] || TA_lili marked as complete%0a#foxend 210506-152451-304020#%0a#foxbegin 210505-221005-918680#%0a* [-05.05.2021 - 15:10-] || TA_mason marked as complete%0a#foxend 210505-221005-918680#%0a[[#historyend]]%0a%0a%25blue%25 '''Comments for student'''%0a%0a[[#comments]]%0a%0a[[#commentsend]]%0a%0a----%0a----%0a!!%0a%0aSummary: \\%0a%0a[[#summary]]%0a%0aDescription and Uses of the Bag of Words Model%0a%0a[[#summaryends]]%0a%0a----%0a[[#content]]%0a%0aThe Bag of Words model is a data structure that stores a set of words that appear in a text and their corresponding frequencies. Below is an example of how a sentence is transformed into a Bag of Words (BoW). %0a%0a(1) John likes to watch movies. Mary likes movies too.%0a%0aBoW1 = {"John":1,"likes":2,"to":1,"watch":1,"movies":2,"Mary":1,"too":1};%0a%0aSince all information about word order and grammar is lost when words are added to the Bag, its uses are limited. %0a%0aHowever, frequency on its own can still be useful in some cases. For example, the Bag of Words model is often used for spam filtering. This is accomplished by putting specific words from thousands of spam emails into a Bag of Words. Then the spam filter simply checks if words in the email are also in the Bag.%0a%0aOne challenge of this is example is knowing which words should be put in the bag. For example; prepositions and determiners are not good indicators of spam. %0a%0aAnother version of the Bag of Words is called the bigram model that stores sets of two consecutive words. An example is shown below.%0a%0a[%0a    "John likes",%0a    "likes to",%0a    "to watch",%0a    "watch movies",%0a    "Mary likes",%0a    "likes movies",%0a    "movies too",%0a]%0a%0aThis model can be more useful because it stores the syntax of a text to a degree. For example, from the bigram example of above we can see that "likes" can take a prepositional phrase complement headed by "to". Also, "likes" can take a CNP complement "movies". %0a%0aWith regards to the spam filtering example, a bigram model can be more useful than a Bag of Words because it is more sensitive to the grammar used in spam emails. In some cases, spam emails are in a language the spammer is not fluent in, so he will use improper grammar. A Bag of Words model may not detect this difference, but a bigram model may.%0a%0aSource: https://en.wikipedia.org/wiki/Bag-of-words_model%0a%0a%0a%0a%0a%0a[[#contentends]]%0a----%0a(:GradedBy: TA_lili:)
time=1620314691
author:1620314691=TA_lili
diff:1620314691:1620252604:=18,20d17%0a%3c #foxbegin 210506-152451-304020#%0a%3c * [-06.05.2021 - 08:24-] || TA_lili marked as complete%0a%3c #foxend 210506-152451-304020#%0a83c80%0a%3c (:GradedBy: TA_lili:)%0a\ No newline at end of file%0a---%0a> (:GradedBy: TA_mason:)%0a\ No newline at end of file%0a
host:1620314691=74.111.97.246
author:1620252604=TA_mason
diff:1620252604:1619914459:=7,8c7,8%0a%3c :Status: complete%0a%3c %0a---%0a> :Status: ungraded%0a> %0a18,20c18%0a%3c #foxbegin 210505-221005-918680#%0a%3c * [-05.05.2021 - 15:10-] || TA_mason marked as complete%0a%3c #foxend 210505-221005-918680#%0a---%0a> %0a79,80c77%0a%3c ----%0a%3c (:GradedBy: TA_mason:)%0a\ No newline at end of file%0a---%0a> ----%0a\ No newline at end of file%0a
host:1620252604=74.98.251.138
author:1619914459=sebastiang
diff:1619914459:1619914459:=1,77d0%0a%3c :Category: Varia%0a%3c :Essential: {Category.Varia$:essential}%0a%3c :Title: Bag of Words Model in Natural Language Processing%0a%3c :Author: sebastiang%0a%3c :Section: F%0a%3c :Completed: 01.05.2021 - 17:14%0a%3c :Status: ungraded%0a%3c %0a%3c (:foxform Site.FoxForms#gradeitem:)%0a%3c (:foxform Site.FoxForms#gradingcomment:)%0a%3c %0a%3c ----%0a%3c %0a%3c %0a%3c %25blue%25 '''Grading History'''%0a%3c %0a%3c [[#history]]%0a%3c %0a%3c [[#historyend]]%0a%3c %0a%3c %25blue%25 '''Comments for student'''%0a%3c %0a%3c [[#comments]]%0a%3c %0a%3c [[#commentsend]]%0a%3c %0a%3c ----%0a%3c ----%0a%3c !!%0a%3c %0a%3c Summary: \\%0a%3c %0a%3c [[#summary]]%0a%3c %0a%3c Description and Uses of the Bag of Words Model%0a%3c %0a%3c [[#summaryends]]%0a%3c %0a%3c ----%0a%3c [[#content]]%0a%3c %0a%3c The Bag of Words model is a data structure that stores a set of words that appear in a text and their corresponding frequencies. Below is an example of how a sentence is transformed into a Bag of Words (BoW). %0a%3c %0a%3c (1) John likes to watch movies. Mary likes movies too.%0a%3c %0a%3c BoW1 = {"John":1,"likes":2,"to":1,"watch":1,"movies":2,"Mary":1,"too":1};%0a%3c %0a%3c Since all information about word order and grammar is lost when words are added to the Bag, its uses are limited. %0a%3c %0a%3c However, frequency on its own can still be useful in some cases. For example, the Bag of Words model is often used for spam filtering. This is accomplished by putting specific words from thousands of spam emails into a Bag of Words. Then the spam filter simply checks if words in the email are also in the Bag.%0a%3c %0a%3c One challenge of this is example is knowing which words should be put in the bag. For example; prepositions and determiners are not good indicators of spam. %0a%3c %0a%3c Another version of the Bag of Words is called the bigram model that stores sets of two consecutive words. An example is shown below.%0a%3c %0a%3c [%0a%3c     "John likes",%0a%3c     "likes to",%0a%3c     "to watch",%0a%3c     "watch movies",%0a%3c     "Mary likes",%0a%3c     "likes movies",%0a%3c     "movies too",%0a%3c ]%0a%3c %0a%3c This model can be more useful because it stores the syntax of a text to a degree. For example, from the bigram example of above we can see that "likes" can take a prepositional phrase complement headed by "to". Also, "likes" can take a CNP complement "movies". %0a%3c %0a%3c With regards to the spam filtering example, a bigram model can be more useful than a Bag of Words because it is more sensitive to the grammar used in spam emails. In some cases, spam emails are in a language the spammer is not fluent in, so he will use improper grammar. A Bag of Words model may not detect this difference, but a bigram model may.%0a%3c %0a%3c Source: https://en.wikipedia.org/wiki/Bag-of-words_model%0a%3c %0a%3c %0a%3c %0a%3c %0a%3c %0a%3c [[#contentends]]%0a%3c ----%0a\ No newline at end of file%0a
host:1619914459=128.237.82.2
