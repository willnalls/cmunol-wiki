version=pmwiki-2.2.130 ordered=1 urlencoded=1
agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36
author=jjmonroe
charset=UTF-8
csum=
ctime=1619289473
host=73.174.7.241
name=Articles.DiscriminationOfAAVEInSocialMediaFilteringTechnology
rev=4
targets=GradebookArticles.DiscriminationOfAAVEInSocialMediaFilteringTechnology,Articles.DiscriminationOfAAVEInSocialMediaFilteringTechnology,Category.LinguisticDiscriminationInTheWild
text=(:if authgroup @tas:)%0a%0a(:toggle id=gi show="show grading interface" hide="hide grading interface":)%0a%0a>>id=gi%3c%3c%0a%0a[[GradebookArticles.{$Name}|See article in gradebook]] \\%0a[[{$FullName}?action=diff|See all changes to article]]%0a%0a(:foxform Site.FoxForms#gradeitem:)%0a%0a(:foxform Site.FoxForms#gradingcomment:)%0a%0a>>%3c%3c%0a%0a----%0a%0a(:ifend:)%0a%0a[[!LinguisticDiscriminationInTheWild]]%0a%0a!Discrimination of AAVE in social media filtering technology%0a%0a:Author: ZoeC%0a%0a'''Summary:''' \\%0a%0a[[#summary]]%0a%0aThe app Gobo has a "rudeness filter" designed to make posts from specifically women of color disappear.%0a%0a[[#summaryends]]%0a%0a----%0a[[#content]]%0a%0aArticle: https://peopleofcolorintech.com/articles/how-automated-tools-discriminate-against-black-language/%0a%0aGobo is a filtering app for social media users, where the aim of the app is to make social media more comfortable for people according to how they want their feed filtered. There are six categories for filtering someone’s feed: politics, seriousness, rudeness, gender, brands, and obscurity. When a user selects one of these categories, Gobo will filter it out and tell them why. The writer of this article talks specifically about the rudeness filter, and how when this is turned on, the posts from women of color start to disappear. %0a%0aPosts that were marked as “very rude” were ones like: “You in yo bag sis! You betta do it sis!!!” “You are a dope ass woman”, even though they are objectively positive statements. This shows how machine learning is programmed to discern AAVE automatically as rude. For example, the deletion of the verb “are”, the deletion of the final vowel and diphthong of “you”, the shortening of “better” to “betta” (deletion of the [ɹ] ), and the use of slang like “dope ass” are flagged as rude because they are common features of AAVE. %0a%0aThere is also a feature of the rudeness filter which uses AI to analyze a sentence and determine if it is “toxic” or not. The writer of the article tested a number of sentences out in the app, and it overwhelmingly identified AAVE as toxic. The most toxic among the tested sentences was “I am a gay black woman.” (identified as 87%25 toxic), and the least toxic was “I am a man.” (20%25 toxic). This shows how technology is constantly being used to suppress AAVE and reinforce racism.%0a%0a%0a[[#contentends]]%0a----%0a----%0a%0a!!Expansions:%0a[[#expansions]]%0a#foxbegin 210504-194546-16520#%0a(:div1 class=expansionhead:)%0a!!!An alternative perspective with regard to machine learning %0a-> - jjmonroe%0a(:div1end:) %0a>>messageitem%3c%3c %0aDue to the nature of the technology which is being used here, the implied intentionality may be more coincidental. Machine learning algorithms aren't necessarily "programmed" the same way a traditional piece of software is; they are fed a set of data and build a model to predict whether future items match that data. If a machine learning algorithm that was created to distinguish flowers from leaves was later found to do a poor job of labeling a certain type of flower it had little experience dealing with, it wouldn't be the the programmers explicitly discriminating against that type, but a failing of the algorithm. Likewise, it seems wrong to say that 'dope ass' was flagged BECAUSE it was AAVE. Assuming that this was a conscious choice rather than an emergent property of the fact that the dataset the model was trained on (an archive of wikipedia comments) didn't contain much AAVE and the algorithm just didn't know what to do with it. Although the legacy of discrimination in tech goes far back, it seems possible in this case that it may be an unintentional side effect of narrow sampling rather than deliberate exclusion.%0a>>%3c%3c%0a#foxend 210504-194546-16520#%0a[[#expansionsend]]%0a%0a----%0a%25red%25 '''Add an expansion:'''%0a%0a(:if [ exists GradebookExpansions.{$Name}-{$Author} || equal {$Author} {$:Author} ] :)%0a%0a(:else:)%0a%0a(:foxform Site.FoxForms#newexpansion:)%0a%0a(:ifend:)%0a%0a----%0a----%0a%0a!!Comments%0a%0a%0a%0a%0a(:section: E:)%0a(:Category: LinguisticDiscriminationInTheWild:)
time=1620157545
author:1620157545=jjmonroe
diff:1620157545:1620156888:=52,60c52%0a%3c #foxbegin 210504-194546-16520#%0a%3c (:div1 class=expansionhead:)%0a%3c !!!An alternative perspective with regard to machine learning %0a%3c -> - jjmonroe%0a%3c (:div1end:) %0a%3c >>messageitem%3c%3c %0a%3c Due to the nature of the technology which is being used here, the implied intentionality may be more coincidental. Machine learning algorithms aren't necessarily "programmed" the same way a traditional piece of software is; they are fed a set of data and build a model to predict whether future items match that data. If a machine learning algorithm that was created to distinguish flowers from leaves was later found to do a poor job of labeling a certain type of flower it had little experience dealing with, it wouldn't be the the programmers explicitly discriminating against that type, but a failing of the algorithm. Likewise, it seems wrong to say that 'dope ass' was flagged BECAUSE it was AAVE. Assuming that this was a conscious choice rather than an emergent property of the fact that the dataset the model was trained on (an archive of wikipedia comments) didn't contain much AAVE and the algorithm just didn't know what to do with it. Although the legacy of discrimination in tech goes far back, it seems possible in this case that it may be an unintentional side effect of narrow sampling rather than deliberate exclusion.%0a%3c >>%3c%3c%0a%3c #foxend 210504-194546-16520#%0a---%0a> %0a
host:1620157545=73.174.7.241
author:1620156888=nithyas
diff:1620156888:1619306397:=37,45c37,45%0a%3c Article: https://peopleofcolorintech.com/articles/how-automated-tools-discriminate-against-black-language/%0a%3c %0a%3c Gobo is a filtering app for social media users, where the aim of the app is to make social media more comfortable for people according to how they want their feed filtered. There are six categories for filtering someone’s feed: politics, seriousness, rudeness, gender, brands, and obscurity. When a user selects one of these categories, Gobo will filter it out and tell them why. The writer of this article talks specifically about the rudeness filter, and how when this is turned on, the posts from women of color start to disappear. %0a%3c %0a%3c Posts that were marked as “very rude” were ones like: “You in yo bag sis! You betta do it sis!!!” “You are a dope ass woman”, even though they are objectively positive statements. This shows how machine learning is programmed to discern AAVE automatically as rude. For example, the deletion of the verb “are”, the deletion of the final vowel and diphthong of “you”, the shortening of “better” to “betta” (deletion of the [ɹ] ), and the use of slang like “dope ass” are flagged as rude because they are common features of AAVE. %0a%3c %0a%3c There is also a feature of the rudeness filter which uses AI to analyze a sentence and determine if it is “toxic” or not. The writer of the article tested a number of sentences out in the app, and it overwhelmingly identified AAVE as toxic. The most toxic among the tested sentences was “I am a gay black woman.” (identified as 87%25 toxic), and the least toxic was “I am a man.” (20%25 toxic). This shows how technology is constantly being used to suppress AAVE and reinforce racism.%0a%3c %0a%3c %0a---%0a> Article: https://peopleofcolorintech.com/articles/how-automated-tools-discriminate-against-black-language/%0a> %0a> Gobo is a filtering app for social media users, where the aim of the app is to make social media more comfortable for people according to how they want their feed filtered. There are six categories for filtering someone’s feed: politics, seriousness, rudeness, gender, brands, and obscurity. When a user selects one of these categories, Gobo will filter it out and tell them why. The writer of this article talks specifically about the rudeness filter, and how when this is turned on, the posts from women of color start to disappear. %0a> %0a> Posts that were marked as “very rude” were ones like: “You in yo bag sis! You betta do it sis!!!” “You are a dope ass woman”, even though they are objectively positive statements. This shows how machine learning is programmed to discern AAVE automatically as rude. For example, the deletion of the verb “are”, the deletion of the final vowel and diphthong of “you”, the shortening of “better” to “betta” (deletion of the [ɹ] ), and the use of slang like “dope ass” are flagged as rude because they are common features of AAVE. %0a> %0a> There is also a feature of the rudeness filter which uses AI to analyze a sentence and determine if it is “toxic” or not. The writer of the article tested a number of sentences out in the app, and it overwhelmingly identified AAVE as toxic. The most toxic among the tested sentences was “I am a gay black woman.” (identified as 87%25 toxic), and the least toxic was “I am a man.” (20%25 toxic). This shows how technology is constantly being used to suppress AAVE and reinforce racism.%0a> %0a> %0a70c70,87%0a%3c %0a---%0a> #foxbegin 210424-231957-758180#%0a> (:div1 class=messagehead:)%0a> >>rfloat%3c%3c   %0a> [-24.04.2021 - 16:19-] &nbsp; %0a> >>%3c%3c%0a> (:if1 authgroup @tas:)%0a> >>rfloat%3c%3c%0a> {[foxdelrange button 210424-231957-758180 {$FullName} ]}%0a> >>%3c%3c%0a> (:if1end:)%0a> !!!!!jjmonroe%0a> (:div1end:) %0a> >>messageitem%3c%3c %0a> ''''''%0a> >>messageitem%3c%3c%0a> The implied intentionality here is weird. Machine learning algorithms aren't necessarily "programmed" the same way a traditional piece of software is; they are fed a set of data and build a model to predict whether future items match that data. If a machine learning algorithm that was created to distinguish flowers from leaves was later found to do a poor job of labeling a certain type of flower it had little experience dealing with, it wouldn't be the the programmers explicitly discriminating against that type, but a failing of the algorithm. Likewise, it seems wrong to say that 'dope ass' was flagged BECAUSE it was AAVE. Assuming that this was a conscious choice rather than an emergent property of the fact that the dataset the model was trained on (an archive of wikipedia comments) didn't contain much AAVE and the algorithm just didn't know what to do with it. %0a> >>%3c%3c%0a> #foxend 210424-231957-758180#%0a
host:1620156888=74.109.239.200
author:1619306397=jjmonroe
diff:1619306397:1619289473:=70,87d69%0a%3c #foxbegin 210424-231957-758180#%0a%3c (:div1 class=messagehead:)%0a%3c >>rfloat%3c%3c   %0a%3c [-24.04.2021 - 16:19-] &nbsp; %0a%3c >>%3c%3c%0a%3c (:if1 authgroup @tas:)%0a%3c >>rfloat%3c%3c%0a%3c {[foxdelrange button 210424-231957-758180 {$FullName} ]}%0a%3c >>%3c%3c%0a%3c (:if1end:)%0a%3c !!!!!jjmonroe%0a%3c (:div1end:) %0a%3c >>messageitem%3c%3c %0a%3c ''''''%0a%3c >>messageitem%3c%3c%0a%3c The implied intentionality here is weird. Machine learning algorithms aren't necessarily "programmed" the same way a traditional piece of software is; they are fed a set of data and build a model to predict whether future items match that data. If a machine learning algorithm that was created to distinguish flowers from leaves was later found to do a poor job of labeling a certain type of flower it had little experience dealing with, it wouldn't be the the programmers explicitly discriminating against that type, but a failing of the algorithm. Likewise, it seems wrong to say that 'dope ass' was flagged BECAUSE it was AAVE. Assuming that this was a conscious choice rather than an emergent property of the fact that the dataset the model was trained on (an archive of wikipedia comments) didn't contain much AAVE and the algorithm just didn't know what to do with it. %0a%3c >>%3c%3c%0a%3c #foxend 210424-231957-758180#%0a
host:1619306397=73.174.7.241
author:1619289473=ZoeC
diff:1619289473:1619289473:=1,74d0%0a%3c (:if authgroup @tas:)%0a%3c %0a%3c (:toggle id=gi show="show grading interface" hide="hide grading interface":)%0a%3c %0a%3c >>id=gi%3c%3c%0a%3c %0a%3c [[GradebookArticles.{$Name}|See article in gradebook]] \\%0a%3c [[{$FullName}?action=diff|See all changes to article]]%0a%3c %0a%3c (:foxform Site.FoxForms#gradeitem:)%0a%3c %0a%3c (:foxform Site.FoxForms#gradingcomment:)%0a%3c %0a%3c >>%3c%3c%0a%3c %0a%3c ----%0a%3c %0a%3c (:ifend:)%0a%3c %0a%3c [[!LinguisticDiscriminationInTheWild]]%0a%3c %0a%3c !Discrimination of AAVE in social media filtering technology%0a%3c %0a%3c :Author: ZoeC%0a%3c %0a%3c '''Summary:''' \\%0a%3c %0a%3c [[#summary]]%0a%3c %0a%3c The app Gobo has a "rudeness filter" designed to make posts from specifically women of color disappear.%0a%3c %0a%3c [[#summaryends]]%0a%3c %0a%3c ----%0a%3c [[#content]]%0a%3c %0a%3c Article: https://peopleofcolorintech.com/articles/how-automated-tools-discriminate-against-black-language/%0a%3c %0a%3c Gobo is a filtering app for social media users, where the aim of the app is to make social media more comfortable for people according to how they want their feed filtered. There are six categories for filtering someone’s feed: politics, seriousness, rudeness, gender, brands, and obscurity. When a user selects one of these categories, Gobo will filter it out and tell them why. The writer of this article talks specifically about the rudeness filter, and how when this is turned on, the posts from women of color start to disappear. %0a%3c %0a%3c Posts that were marked as “very rude” were ones like: “You in yo bag sis! You betta do it sis!!!” “You are a dope ass woman”, even though they are objectively positive statements. This shows how machine learning is programmed to discern AAVE automatically as rude. For example, the deletion of the verb “are”, the deletion of the final vowel and diphthong of “you”, the shortening of “better” to “betta” (deletion of the [ɹ] ), and the use of slang like “dope ass” are flagged as rude because they are common features of AAVE. %0a%3c %0a%3c There is also a feature of the rudeness filter which uses AI to analyze a sentence and determine if it is “toxic” or not. The writer of the article tested a number of sentences out in the app, and it overwhelmingly identified AAVE as toxic. The most toxic among the tested sentences was “I am a gay black woman.” (identified as 87%25 toxic), and the least toxic was “I am a man.” (20%25 toxic). This shows how technology is constantly being used to suppress AAVE and reinforce racism.%0a%3c %0a%3c %0a%3c [[#contentends]]%0a%3c ----%0a%3c ----%0a%3c %0a%3c !!Expansions:%0a%3c [[#expansions]]%0a%3c %0a%3c [[#expansionsend]]%0a%3c %0a%3c ----%0a%3c %25red%25 '''Add an expansion:'''%0a%3c %0a%3c (:if [ exists GradebookExpansions.{$Name}-{$Author} || equal {$Author} {$:Author} ] :)%0a%3c %0a%3c (:else:)%0a%3c %0a%3c (:foxform Site.FoxForms#newexpansion:)%0a%3c %0a%3c (:ifend:)%0a%3c %0a%3c ----%0a%3c ----%0a%3c %0a%3c !!Comments%0a%3c %0a%3c %0a%3c %0a%3c (:section: E:)%0a%3c (:Category: LinguisticDiscriminationInTheWild:)%0a\ No newline at end of file%0a
host:1619289473=128.2.149.254
