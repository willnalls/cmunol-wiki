version=pmwiki-2.2.130 ordered=1 urlencoded=1
agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:87.0) Gecko/20100101 Firefox/87.0
author=AlexX
charset=UTF-8
csum=
ctime=1618789392
host=71.206.238.88
name=Articles.NeuralMachineTranslation
rev=3
targets=GradebookArticles.NeuralMachineTranslation,Articles.NeuralMachineTranslation,Category.Varia
text=(:if authgroup @tas:)%0a%0a(:toggle id=gi show="show grading interface" hide="hide grading interface":)%0a%0a>>id=gi%3c%3c%0a%0a[[GradebookArticles.{$Name}|See article in gradebook]] \\%0a[[{$FullName}?action=diff|See all changes to article]]%0a%0a(:foxform Site.FoxForms#gradeitem:)%0a%0a(:foxform Site.FoxForms#gradingcomment:)%0a%0a>>%3c%3c%0a%0a----%0a%0a(:ifend:)%0a%0a[[!Varia]]%0a%0a!Neural Machine Translation%0a%0a:Author: aryans%0a%0a'''Summary:''' \\%0a%0a[[#summary]]%0a%0aAn overview of neural machine translation (NMT) and a summary of research analyzing the parts of pretrained NMT models %0a%0a[[#summaryends]]%0a%0a----%0a[[#content]]%0a%0aMachine translation (MT) is a subfield of computational linguistics that seeks to create algorithms that translate from one language to another. Today, we will see a change that has taken place in MT research in recent years, a problem that this change poses, and how some researchers have attempted to address it.  %0a%0aIn the past, MT has usually been done using separate components that each construct one piece of the puzzle. For example, one algorithm might directly translate the words, another might apply appropriate affixes to the translated words to create agreement in the translated language, yet another might reorder the words appropriately, and so on.  Usually, these sub-algorithms use statistical methods relying on examples of already translated text. After they are created, the sub-algorithms are concatenated to create the final MT algorithm.%0a%0aHowever, researchers have recently shifted to using deep neural networks for the MT task--this is called neural machine translation (NMT). Today's NMT models, instead of dividing translation into multiple steps, are trained end-to-end. This means that they use a single deep neural net to do the entire translation, usually sentence by sentence, and that this neural net is trained all together, rather than different parts of it being trained as sub-algorithms. %0a%0aWhile this method is promising, a notable downside of NMT is that researchers are less sure about which pieces of the translation process are being done where in the NMT algorithm. Information like this provides valuable feedback on how to improve the algorithms. Therefore, a team of researchers out of MIT and Qatar Computing Research Institute have analyzed the representations of the data at various points in the NMT to ascertain whether they contain any linguistic information that common sense would suggest, such as information about the morphology, syntax, and semantics of the data being translated. The researchers concluded that morphological information is usually learned at lower layers, earlier in the neural net, while syntax and semantics are learned at higher layers, later in the algorithm.  %0a%0aAnother thing the researchers looked at is how the structure of the input data affects the efficacy of the final algorithm. Here, they concluded that representing the data as individual characters usually produced models better at identifying unfamiliar words, and that this method should be used "when translating morphologically rich languages such as Czech" (41). However, using subword units, rather than individual characters, was better for translation between "syntactically divergent language pairs such as German-English". This is because if two languages have notably different syntax, translation relies more on understanding the relationship between two words that could be far apart in one language but close together when you translate into the other, and this is somehow harder to capture when the input is passed in character by character. %0a%0aSources: %0a%0a- Belinkov et al. "On the Linguistic Representation Power of Neural Machine Translation Models" (2019)%0a%0a- Wikipedia: %0ahttps://en.wikipedia.org/wiki/Machine_translation%0ahttps://en.wikipedia.org/wiki/Neural_machine_translation%0a%0a%0a[[#contentends]]%0a----%0a----%0a%0a!!Expansions:%0a[[#expansions]]%0a#foxbegin 210429-131741-960320#%0a(:div1 class=expansionhead:)%0a!!!Tokenization %0a-> - EthanW%0a(:div1end:) %0a>>messageitem%3c%3c %0aIn the last paragraph, you mentioned that the structure of input data would impact the neural translator's performance. In fact, converting strings of sentences into word-like units as input to the model involves a process called tokenization. More formally, tokenization splits a phrase, sentence, paragraph, or an entire text document into smaller units, such as individual words or terms. For language like English, it is usually easy to identify units of a sentence by blank space. As for Mandarin Chinese, however, it is harder to determine where to make a split. %0a%0aIn many state-of-the-art computational linguistic models, tokenization is data-driven based on frequency, dependency, etc. There are also many well-documented implementations that enable typical users to perform tokenization very efficiently (see "source" for more details). %0a%0aSource: https://en.wikipedia.org/wiki/Lexical_analysis#Tokenization; %0ahttps://github.com/huggingface/tokenizers; %0ahttps://www.analyticsvidhya.com/blog/2019/07/how-get-started-nlp-6-unique-ways-perform-tokenization/%0a>>%3c%3c%0a#foxend 210429-131741-960320#%0a#foxbegin 210508-000849-797990#%0a(:div1 class=expansionhead:)%0a!!!Explainability in NMT %0a-> - AlexX%0a(:div1end:) %0a>>messageitem%3c%3c %0aWhile like you mention, end-to-end NMT models are in general quite difficult to explain, it turns out that certain "attentional" models allow us to glean quite a bit of insight into their operation. Essentially, attention is a technique where the model learns to "pay attention" to various parts of the input sentence (in language A) as it generates the output sentence (in language B). %0a%0aHere's a graphic illustrating attention in English to French translation. The brighter a spot is, the more attention is paid to it:%0ahttps://paperswithcode.com/media/methods/Screen_Shot_2020-05-24_at_7.58.36_PM.png%0a%0aAs we might expect, when the model generates a word in French, it pays the most attention to the corresponding English word in the input sentence (i.e. when generating européenne, European is the word that is paid the most attention). This seemingly implies that our attentional mechanism has some sense of the syntactic structure of both languages. Of course, this nice pattern is aided by the fact that French and English are fairly similar languages; I have no idea if it would also appear when translating from English to a language with wildly different syntax. %0a%0aSource:%0a%0ahttps://arxiv.org/abs/1409.0473%0a%0ahttps://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html%0a>>%3c%3c%0a#foxend 210508-000849-797990#%0a[[#expansionsend]]%0a%0a----%0a%25red%25 '''Add an expansion:'''%0a%0a(:if [ exists GradebookExpansions.{$Name}-{$Author} || equal {$Author} {$:Author} ] :)%0a%0a(:else:)%0a%0a(:foxform Site.FoxForms#newexpansion:)%0a%0a(:ifend:)%0a%0a----%0a----%0a%0a!!Comments%0a%0a%0a%0a(:section: B:)%0a(:Category: Varia:)
time=1620432529
author:1620432529=AlexX
diff:1620432529:1619702260:=77,96d76%0a%3c #foxbegin 210508-000849-797990#%0a%3c (:div1 class=expansionhead:)%0a%3c !!!Explainability in NMT %0a%3c -> - AlexX%0a%3c (:div1end:) %0a%3c >>messageitem%3c%3c %0a%3c While like you mention, end-to-end NMT models are in general quite difficult to explain, it turns out that certain "attentional" models allow us to glean quite a bit of insight into their operation. Essentially, attention is a technique where the model learns to "pay attention" to various parts of the input sentence (in language A) as it generates the output sentence (in language B). %0a%3c %0a%3c Here's a graphic illustrating attention in English to French translation. The brighter a spot is, the more attention is paid to it:%0a%3c https://paperswithcode.com/media/methods/Screen_Shot_2020-05-24_at_7.58.36_PM.png%0a%3c %0a%3c As we might expect, when the model generates a word in French, it pays the most attention to the corresponding English word in the input sentence (i.e. when generating européenne, European is the word that is paid the most attention). This seemingly implies that our attentional mechanism has some sense of the syntactic structure of both languages. Of course, this nice pattern is aided by the fact that French and English are fairly similar languages; I have no idea if it would also appear when translating from English to a language with wildly different syntax. %0a%3c %0a%3c Source:%0a%3c %0a%3c https://arxiv.org/abs/1409.0473%0a%3c %0a%3c https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html%0a%3c >>%3c%3c%0a%3c #foxend 210508-000849-797990#%0a
host:1620432529=71.206.238.88
author:1619702260=EthanW
diff:1619702260:1618789392:=62,76c62%0a%3c #foxbegin 210429-131741-960320#%0a%3c (:div1 class=expansionhead:)%0a%3c !!!Tokenization %0a%3c -> - EthanW%0a%3c (:div1end:) %0a%3c >>messageitem%3c%3c %0a%3c In the last paragraph, you mentioned that the structure of input data would impact the neural translator's performance. In fact, converting strings of sentences into word-like units as input to the model involves a process called tokenization. More formally, tokenization splits a phrase, sentence, paragraph, or an entire text document into smaller units, such as individual words or terms. For language like English, it is usually easy to identify units of a sentence by blank space. As for Mandarin Chinese, however, it is harder to determine where to make a split. %0a%3c %0a%3c In many state-of-the-art computational linguistic models, tokenization is data-driven based on frequency, dependency, etc. There are also many well-documented implementations that enable typical users to perform tokenization very efficiently (see "source" for more details). %0a%3c %0a%3c Source: https://en.wikipedia.org/wiki/Lexical_analysis#Tokenization; %0a%3c https://github.com/huggingface/tokenizers; %0a%3c https://www.analyticsvidhya.com/blog/2019/07/how-get-started-nlp-6-unique-ways-perform-tokenization/%0a%3c >>%3c%3c%0a%3c #foxend 210429-131741-960320#%0a---%0a> %0a
host:1619702260=58.247.22.142
author:1618789392=aryans
diff:1618789392:1618789392:=1,84d0%0a%3c (:if authgroup @tas:)%0a%3c %0a%3c (:toggle id=gi show="show grading interface" hide="hide grading interface":)%0a%3c %0a%3c >>id=gi%3c%3c%0a%3c %0a%3c [[GradebookArticles.{$Name}|See article in gradebook]] \\%0a%3c [[{$FullName}?action=diff|See all changes to article]]%0a%3c %0a%3c (:foxform Site.FoxForms#gradeitem:)%0a%3c %0a%3c (:foxform Site.FoxForms#gradingcomment:)%0a%3c %0a%3c >>%3c%3c%0a%3c %0a%3c ----%0a%3c %0a%3c (:ifend:)%0a%3c %0a%3c [[!Varia]]%0a%3c %0a%3c !Neural Machine Translation%0a%3c %0a%3c :Author: aryans%0a%3c %0a%3c '''Summary:''' \\%0a%3c %0a%3c [[#summary]]%0a%3c %0a%3c An overview of neural machine translation (NMT) and a summary of research analyzing the parts of pretrained NMT models %0a%3c %0a%3c [[#summaryends]]%0a%3c %0a%3c ----%0a%3c [[#content]]%0a%3c %0a%3c Machine translation (MT) is a subfield of computational linguistics that seeks to create algorithms that translate from one language to another. Today, we will see a change that has taken place in MT research in recent years, a problem that this change poses, and how some researchers have attempted to address it.  %0a%3c %0a%3c In the past, MT has usually been done using separate components that each construct one piece of the puzzle. For example, one algorithm might directly translate the words, another might apply appropriate affixes to the translated words to create agreement in the translated language, yet another might reorder the words appropriately, and so on.  Usually, these sub-algorithms use statistical methods relying on examples of already translated text. After they are created, the sub-algorithms are concatenated to create the final MT algorithm.%0a%3c %0a%3c However, researchers have recently shifted to using deep neural networks for the MT task--this is called neural machine translation (NMT). Today's NMT models, instead of dividing translation into multiple steps, are trained end-to-end. This means that they use a single deep neural net to do the entire translation, usually sentence by sentence, and that this neural net is trained all together, rather than different parts of it being trained as sub-algorithms. %0a%3c %0a%3c While this method is promising, a notable downside of NMT is that researchers are less sure about which pieces of the translation process are being done where in the NMT algorithm. Information like this provides valuable feedback on how to improve the algorithms. Therefore, a team of researchers out of MIT and Qatar Computing Research Institute have analyzed the representations of the data at various points in the NMT to ascertain whether they contain any linguistic information that common sense would suggest, such as information about the morphology, syntax, and semantics of the data being translated. The researchers concluded that morphological information is usually learned at lower layers, earlier in the neural net, while syntax and semantics are learned at higher layers, later in the algorithm.  %0a%3c %0a%3c Another thing the researchers looked at is how the structure of the input data affects the efficacy of the final algorithm. Here, they concluded that representing the data as individual characters usually produced models better at identifying unfamiliar words, and that this method should be used "when translating morphologically rich languages such as Czech" (41). However, using subword units, rather than individual characters, was better for translation between "syntactically divergent language pairs such as German-English". This is because if two languages have notably different syntax, translation relies more on understanding the relationship between two words that could be far apart in one language but close together when you translate into the other, and this is somehow harder to capture when the input is passed in character by character. %0a%3c %0a%3c Sources: %0a%3c %0a%3c - Belinkov et al. "On the Linguistic Representation Power of Neural Machine Translation Models" (2019)%0a%3c %0a%3c - Wikipedia: %0a%3c https://en.wikipedia.org/wiki/Machine_translation%0a%3c https://en.wikipedia.org/wiki/Neural_machine_translation%0a%3c %0a%3c %0a%3c [[#contentends]]%0a%3c ----%0a%3c ----%0a%3c %0a%3c !!Expansions:%0a%3c [[#expansions]]%0a%3c %0a%3c [[#expansionsend]]%0a%3c %0a%3c ----%0a%3c %25red%25 '''Add an expansion:'''%0a%3c %0a%3c (:if [ exists GradebookExpansions.{$Name}-{$Author} || equal {$Author} {$:Author} ] :)%0a%3c %0a%3c (:else:)%0a%3c %0a%3c (:foxform Site.FoxForms#newexpansion:)%0a%3c %0a%3c (:ifend:)%0a%3c %0a%3c ----%0a%3c ----%0a%3c %0a%3c !!Comments%0a%3c %0a%3c %0a%3c %0a%3c (:section: B:)%0a%3c (:Category: Varia:)%0a\ No newline at end of file%0a
host:1618789392=184.190.142.79
